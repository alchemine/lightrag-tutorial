{
  "chunk-8a06df51e07fa7bf7dea2bbbc5e7cd98": {
    "tokens": 1200,
    "content": "From Local to Global: A Graph RAG Approach to\nQuery-Focused Summarization\nDarren Edge1† Ha Trinh1† Newman Cheng2 Joshua Bradley2 Alex Chao3\nApurva Mody3 Steven Truitt2\nJonathan Larson1\n1Microsoft Research\n2Microsoft Strategic Missions and Technologies\n3Microsoft Office of the CTO\n{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso}\n@microsoft.com\n†These authors contributed equally to this work\nAbstract\nThe use of retrieval-augmented generation (RAG) to retrieve relevant informa-\ntion from an external knowledge source enables large language models (LLMs)\nto answer questions over private and/or previously unseen document collections.\nHowever, RAG fails on global questions directed at an entire text corpus, such\nas “What are the main themes in the dataset?”, since this is inherently a query-\nfocused summarization (QFS) task, rather than an explicit retrieval task. Prior\nQFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\nRAG systems. To combine the strengths of these contrasting methods, we propose\na Graph RAG approach to question answering over private text corpora that scales\nwith both the generality of user questions and the quantity of source text to be in-\ndexed. Our approach uses an LLM to build a graph-based text index in two stages:\nfirst to derive an entity knowledge graph from the source documents, then to pre-\ngenerate community summaries for all groups of closely-related entities. Given a\nquestion, each community summary is used to generate a partial response, before\nall partial responses are again summarized in a final response to the user. For a\nclass of global sensemaking questions over datasets in the 1 million token range,\nwe show that Graph RAG leads to substantial improvements over a na ¨ıve RAG\nbaseline for both the comprehensiveness and diversity of generated answers. An\nopen-source, Python-based implementation of both global and local Graph RAG\napproaches is forthcoming at https://aka.ms/graphrag.\n1 Introduction\nHuman endeavors across a range of domains rely on our ability to read and reason about large\ncollections of documents, often reaching conclusions that go beyond anything stated in the source\ntexts themselves. With the emergence of large language models (LLMs), we are already witnessing\nattempts to automate human-like sensemaking in complex domains like scientific discovery (Mi-\ncrosoft, 2023) and intelligence analysis (Ranade and Joshi, 2023), where sensemaking is defined as\nPreprint. Under review.\narXiv:2404.16130v1  [cs.CL]  24 Apr 2024\nSource Documents\nText Chunks\ntext extraction\nand chunking\nElement Instances\ndomain-tailored\nsummarization\nElement Summaries\ndomain-tailored\nsummarization\nGraph Communities\ncommunity\ndetection\nCommunity Summaries\ndomain-tailored\nsummarization\nCommunity Answers\nquery-focused\nsummarization\nGlobal Answer\nquery-focused\nsummarization\nIndexing Time Query TimePipeline Stage\nFigure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This\nindex spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that have\nbeen detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset.\nCommunity detection (e.g., Leiden, Traag et al., 2019) is used to partition the graph index into\ngroups of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both index-\ning time and query time. The “global answer” to a given query is produced using a final round of\nquery-focused summarization over all community summaries reporting relevance to that query.\n“a motivated, continuous effort to understand connections (which can be among people, places, and\nevents) in order to anticipate their trajectories and act effectively” (Klein et al., 2006a). Supporting\nhuman-led sensemaking over entire text corpora, however, needs a way for people to both apply and\nrefine their mental model of the data (Klein et al., 2006b) by asking questions of a global nature.\nRetrieval-augmented generation (RAG, Lewis et al., 2020) is an established approach to answering\nuser questions over entire datasets, but it is designed for situations where these answers are contained\nlocally within regions of text whose retrieval provides sufficient grounding for the generation task.\nInstead, a more appropriate task framing is query-focused summarization (QFS, Dang, 2006), and in\nparticular, query-focused abstractive summarization that generates natural language summaries and\nnot just concatenated excerpts (Baumel et al., 2018; Laskar et al., 2020; Yao et al., 2017) . In recent\nyears, however, such distinctions between summarization tasks that are abstractive versus extractive,\ngeneric versus query-focused, and single-document versus multi-document, have become less rele-\nvant. While early applications of the transformer architecture showed substantial improvements on\nthe state-of-the-art for all such summarization tasks (Goodwin et al., 2020; Laskar et al., 2022; Liu\nand Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam\net al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) series,\nall",
    "chunk_order_index": 0,
    "full_doc_id": "doc-2967b55fb6e8db0d5c634c9f010180a4"
  },
  "chunk-08d36b76886fe5200846ae27f3daca32": {
    "tokens": 1200,
    "content": "-of-the-art for all such summarization tasks (Goodwin et al., 2020; Laskar et al., 2022; Liu\nand Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam\net al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) series,\nall of which can use in-context learning to summarize any content provided in their context window.\nThe challenge remains, however, for query-focused abstractive summarization over an entire corpus.\nSuch volumes of text can greatly exceed the limits of LLM context windows, and the expansion of\nsuch windows may not be enough given that information can be “lost in the middle” of longer\ncontexts (Kuratov et al., 2024; Liu et al., 2023). In addition, although the direct retrieval of text\nchunks in na¨ıve RAG is likely inadequate for QFS tasks, it is possible that an alternative form of\npre-indexing could support a new RAG approach specifically targeting global summarization.\nIn this paper, we present aGraph RAG approach based on global summarization of an LLM-derived\nknowledge graph (Figure 1). In contrast with related work that exploits the structured retrieval\nand traversal affordances of graph indexes (subsection 4.2), we focus on a previously unexplored\nquality of graphs in this context: their inherent modularity (Newman, 2006) and the ability of com-\nmunity detection algorithms to partition graphs into modular communities of closely-related nodes\n(e.g., Louvain, Blondel et al., 2008; Leiden, Traag et al., 2019). LLM-generated summaries of these\n2\n0 1 2 30\n10000\n20000\n30000\nNumber of gleanings performed\nEntity references detected\n600 chunk size\n1200 chunk size\n2400 chunk size\nFigure 2: How the entity references detected in the HotPotQA dataset (Yang et al., 2018)\nvaries with chunk size and gleanings for our generic entity extraction prompt with gpt-4-turbo.\ncommunity descriptions provide complete coverage of the underlying graph index and the input doc-\numents it represents. Query-focused summarization of an entire corpus is then made possible using\na map-reduce approach: first using each community summary to answer the query independently\nand in parallel, then summarizing all relevant partial answers into a final global answer.\nTo evaluate this approach, we used an LLM to generate a diverse set of activity-centered sense-\nmaking questions from short descriptions of two representative real-world datasets, containing pod-\ncast transcripts and news articles respectively. For the target qualities of comprehensiveness, diver-\nsity, and empowerment (defined in subsection 3.4) that develop understanding of broad issues and\nthemes, we both explore the impact of varying the the hierarchical level of community summaries\nused to answer queries, as well as compare to na ¨ıve RAG and global map-reduce summarization\nof source texts. We show that all global approaches outperform na ¨ıve RAG on comprehensiveness\nand diversity, and that Graph RAG with intermediate- and low-level community summaries shows\nfavorable performance over source text summarization on these same metrics, at lower token costs.\n2 Graph RAG Approach & Pipeline\nWe now unpack the high-level data flow of the Graph RAG approach (Figure 1) and pipeline, de-\nscribing key design parameters, techniques, and implementation details for each step.\n2.1 Source Documents → Text Chunks\nA fundamental design decision is the granularity with which input texts extracted from source doc-\numents should be split into text chunks for processing. In the following step, each of these chunks\nwill be passed to a set of LLM prompts designed to extract the various elements of a graph index.\nLonger text chunks require fewer LLM calls for such extraction, but suffer from the recall degrada-\ntion of longer LLM context windows (Kuratov et al., 2024; Liu et al., 2023). This behavior can be\nobserved in Figure 2 in the case of a single extraction round (i.e., with zero gleanings): on a sample\ndataset (HotPotQA, Yang et al., 2018), using a chunk size of 600 token extracted almost twice as\nmany entity references as when using a chunk size of 2400. While more references are generally\nbetter, any extraction process needs to balance recall and precision for the target activity.\n2.2 Text Chunks → Element Instances\nThe baseline requirement for this step is to identify and extract instances of graph nodes and edges\nfrom each chunk of source text. We do this using a multipart LLM prompt that first identifies all\nentities in the text, including their name, type, and description, before identifying all relationships\nbetween clearly-related entities, including the source and target entities and a description of their\nrelationship. Both kinds of element instance are output in a single list of delimited tuples.\nThe primary opportunity to tailor this prompt to the domain of the document corpus lies in the\nchoice of few-shot examples provided to the LLM for in-context learning (Brown et al., 2020).\n3\nFor example, while our default prompt extracting the broad class of “named entities” like people,\nplaces, and organizations is generally applicable, domains with specialized knowledge (e.g., science,\nmedicine, law) will benefit from few-shot examples specialized to those domains. We also support\na secondary extraction prompt for any additional covariates we would like to associate with the\nextracted node instances. Our default covariate",
    "chunk_order_index": 1,
    "full_doc_id": "doc-2967b55fb6e8db0d5c634c9f010180a4"
  },
  "chunk-4700da6888063f9f62b005679de8d933": {
    "tokens": 1200,
    "content": "LM for in-context learning (Brown et al., 2020).\n3\nFor example, while our default prompt extracting the broad class of “named entities” like people,\nplaces, and organizations is generally applicable, domains with specialized knowledge (e.g., science,\nmedicine, law) will benefit from few-shot examples specialized to those domains. We also support\na secondary extraction prompt for any additional covariates we would like to associate with the\nextracted node instances. Our default covariate prompt aims to extract claims linked to detected\nentities, including the subject, object, type, description, source text span, and start and end dates.\nTo balance the needs of efficiency and quality, we use multiple rounds of “gleanings”, up to a\nspecified maximum, to encourage the LLM to detect any additional entities it may have missed\non prior extraction rounds. This is a multi-stage process in which we first ask the LLM to assess\nwhether all entities were extracted, using a logit bias of 100 to force a yes/no decision. If the LLM\nresponds that entities were missed, then a continuation indicating that “MANY entities were missed\nin the last extraction” encourages the LLM to glean these missing entities. This approach allows us\nto use larger chunk sizes without a drop in quality (Figure 2) or the forced introduction of noise.\n2.3 Element Instances → Element Summaries\nThe use of an LLM to “extract” descriptions of entities, relationships, and claims represented in\nsource texts is already a form of abstractive summarization, relying on the LLM to create inde-\npendently meaningful summaries of concepts that may be implied but not stated by the text itself\n(e.g., the presence of implied relationships). To convert all such instance-level summaries into sin-\ngle blocks of descriptive text for each graph element (i.e., entity node, relationship edge, and claim\ncovariate) requires a further round of LLM summarization over matching groups of instances.\nA potential concern at this stage is that the LLM may not consistently extract references to the\nsame entity in the same text format, resulting in duplicate entity elements and thus duplicate nodes\nin the entity graph. However, since all closely-related “communities” of entities will be detected\nand summarized in the following step, and given that LLMs can understand the common entity\nbehind multiple name variations, our overall approach is resilient to such variations provided there\nis sufficient connectivity from all variations to a shared set of closely-related entities.\nOverall, our use of rich descriptive text for homogeneous nodes in a potentially noisy graph structure\nis aligned with both the capabilities of LLMs and the needs of global, query-focused summarization.\nThese qualities also differentiate our graph index from typical knowledge graphs, which rely on\nconcise and consistent knowledge triples (subject, predicate, object) for downstream reasoning tasks.\n2.4 Element Summaries → Graph Communities\nThe index created in the previous step can be modelled as an homogeneous undirected weighted\ngraph in which entity nodes are connected by relationship edges, with edge weights representing the\nnormalized counts of detected relationship instances. Given such a graph, a variety of community\ndetection algorithms may be used to partition the graph into communities of nodes with stronger\nconnections to one another than to the other nodes in the graph (e.g., see the surveys by Fortu-\nnato, 2010 and Jin et al., 2021). In our pipeline, we use Leiden (Traag et al., 2019) on account of\nits ability to recover hierarchical community structure of large-scale graphs efficiently (Figure 3).\nEach level of this hierarchy provides a community partition that covers the nodes of the graph in a\nmutually-exclusive, collective-exhaustive way, enabling divide-and-conquer global summarization.\n2.5 Graph Communities → Community Summaries\nThe next step is to create report-like summaries of each community in the Leiden hierarchy, using\na method designed to scale to very large datasets. These summaries are independently useful in\ntheir own right as a way to understand the global structure and semantics of the dataset, and may\nthemselves be used to make sense of a corpus in the absence of a question. For example, a user\nmay scan through community summaries at one level looking for general themes of interest, then\nfollow links to the reports at the lower level that provide more details for each of the subtopics. Here,\nhowever, we focus on their utility as part of a graph-based index used for answering global queries.\nCommunity summaries are generated in the following way:\n4\n(a) Root communities at level 0 (b) Sub-communities at level 1\nFigure 3: Graph communities detected using the Leiden algorithm (Traag et al., 2019) over the\nMultiHop-RAG (Tang and Yang, 2024) dataset as indexed. Circles represent entity nodes with size\nproportional to their degree. Node layout was performed via OpenORD (Martin et al., 2011) and\nForce Atlas 2 (Jacomy et al., 2014). Node colors represent entity communities, shown at two levels\nof hierarchical clustering: (a) Level 0, corresponding to the hierarchical partition with maximum\nmodularity, and (b) Level 1, which reveals internal structure within these root-level communities.\n• Leaf-level communities. The element summaries of a leaf-level community (nodes, edges,\ncovariates) are prioritized and then iteratively added to the LLM context window until\nthe token limit is reached. The prioritization is as follows: for each community edge in\ndecreasing order of combined source and target node degree (i.e., overall prominance), add\ndescriptions of the source node, target node, linked covariates, and the edge itself.",
    "chunk_order_index": 2,
    "full_doc_id": "doc-2967b55fb6e8db0d5c634c9f010180a4"
  },
  "chunk-8d50c220259883ba4adef6347651aaea": {
    "tokens": 1200,
    "content": "root-level communities.\n• Leaf-level communities. The element summaries of a leaf-level community (nodes, edges,\ncovariates) are prioritized and then iteratively added to the LLM context window until\nthe token limit is reached. The prioritization is as follows: for each community edge in\ndecreasing order of combined source and target node degree (i.e., overall prominance), add\ndescriptions of the source node, target node, linked covariates, and the edge itself.\n• Higher-level communities. If all element summaries fit within the token limit of the con-\ntext window, proceed as for leaf-level communities and summarize all element summaries\nwithin the community. Otherwise, rank sub-communities in decreasing order of element\nsummary tokens and iteratively substitute sub-community summaries (shorter) for their\nassociated element summaries (longer) until fit within the context window is achieved.\n2.6 Community Summaries → Community Answers → Global Answer\nGiven a user query, the community summaries generated in the previous step can be used to generate\na final answer in a multi-stage process. The hierarchical nature of the community structure also\nmeans that questions can be answered using the community summaries from different levels, raising\nthe question of whether a particular level in the hierarchical community structure offers the best\nbalance of summary detail and scope for general sensemaking questions (evaluated in section 3).\nFor a given community level, the global answer to any user query is generated as follows:\n• Prepare community summaries. Community summaries are randomly shuffled and divided\ninto chunks of pre-specified token size. This ensures relevant information is distributed\nacross chunks, rather than concentrated (and potentially lost) in a single context window.\n• Map community answers. Generate intermediate answers in parallel, one for each chunk.\nThe LLM is also asked to generate a score between 0-100 indicating how helpful the gen-\nerated answer is in answering the target question. Answers with score 0 are filtered out.\n• Reduce to global answer. Intermediate community answers are sorted in descending order\nof helpfulness score and iteratively added into a new context window until the token limit\nis reached. This final context is used to generate the global answer returned to the user.\n5\nDataset Example activity framing and generation of global sensemaking questions\nPodcast\ntranscripts\nUser: A tech journalist looking for insights and trends in the tech industry\nTask: Understanding how tech leaders view the role of policy and regulation\nQuestions:\n1. Which episodes deal primarily with tech policy and government regulation?\n2. How do guests perceive the impact of privacy laws on technology development?\n3. Do any guests discuss the balance between innovation and ethical considerations?\n4. What are the suggested changes to current policies mentioned by the guests?\n5. Are collaborations between tech companies and governments discussed and how?\nNews\narticles\nUser: Educator incorporating current affairs into curricula\nTask: Teaching about health and wellness\nQuestions:\n1. What current topics in health can be integrated into health education curricula?\n2. How do news articles address the concepts of preventive medicine and wellness?\n3. Are there examples of health articles that contradict each other, and if so, why?\n4. What insights can be gleaned about public health priorities based on news coverage?\n5. How can educators use the dataset to highlight the importance of health literacy?\nTable 1: Examples of potential users, tasks, and questions generated by the LLM based on short\ndescriptions of the target datasets. Questions target global understanding rather than specific details.\n3 Evaluation\n3.1 Datasets\nWe selected two datasets in the one million token range, each equivalent to about 10 novels of text\nand representative of the kind of corpora that users may encounter in their real world activities:\n• Podcast transcripts. Compiled transcripts of podcast conversations between Kevin Scott,\nMicrosoft CTO, and other technology leaders (Behind the Tech, Scott, 2024). Size: 1669\n× 600-token text chunks, with 100-token overlaps between chunks (∼1 million tokens).\n• News articles. Benchmark dataset comprising news articles published from September\n2013 to December 2023 in a range of categories, including entertainment, business, sports,\ntechnology, health, and science (MultiHop-RAG; Tang and Yang, 2024). Size: 3197 ×\n600-token text chunks, with 100-token overlaps between chunks (∼1.7 million tokens).\n3.2 Queries\nMany benchmark datasets for open-domain question answering exist, including HotPotQA (Yang\net al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However,\nthe associated question sets target explicit fact retrieval rather than summarization for the purpose\nof data sensemaking, i.e., the process though which people inspect, engage with, and contextualize\ndata within the broader scope of real-world activities (Koesten et al., 2021). Similarly, methods for\nextracting latent summarization queries from source texts also exist (Xu and Lapata, 2021), but such\nextracted questions can target details that betray prior knowledge of the texts.\nTo evaluate the effectiveness of RAG systems for more global sensemaking tasks, we need questions\nthat convey only a high-level understanding of dataset contents, and not the details of specific texts.\nWe used an activity-centered approach to automate the generation of such questions: given a short\ndescription of a dataset, we asked the LLM to identify N potential users and N tasks per user,\nthen for each (user, task) combination, we asked the LLM to generate N questions that require\nunderstanding of the entire corpus. For our evaluation, a value ofN = 5 resulted in 125 test questions\nper dataset. Table 1 shows example questions for each",
    "chunk_order_index": 3,
    "full_doc_id": "doc-2967b55fb6e8db0d5c634c9f010180a4"
  },
  "chunk-18c287f4fb86badedac15c19c5ff2b63": {
    "tokens": 1200,
    "content": "specific texts.\nWe used an activity-centered approach to automate the generation of such questions: given a short\ndescription of a dataset, we asked the LLM to identify N potential users and N tasks per user,\nthen for each (user, task) combination, we asked the LLM to generate N questions that require\nunderstanding of the entire corpus. For our evaluation, a value ofN = 5 resulted in 125 test questions\nper dataset. Table 1 shows example questions for each of the two evaluation datasets.\n6\n3.3 Conditions\nWe compare six different conditions in our analysis, including Graph RAG using four levels of graph\ncommunities (C0, C1, C2, C3), a text summarization method applying our map-reduce approach\ndirectly to source texts (TS), and a na¨ıve “semantic search” RAG approach (SS):\n• CO. Uses root-level community summaries (fewest in number) to answer user queries.\n• C1. Uses high-level community summaries to answer queries. These are sub-communities\nof C0, if present, otherwise C0 communities projected down.\n• C2. Uses intermediate-level community summaries to answer queries. These are sub-\ncommunities of C1, if present, otherwise C1 communities projected down.\n• C3. Uses low-level community summaries (greatest in number) to answer queries. These\nare sub-communities of C2, if present, otherwise C2 communities projected down.\n• TS. The same method as in subsection 2.6, except source texts (rather than community\nsummaries) are shuffled and chunked for the map-reduce summarization stages.\n• SS. An implementation of na¨ıve RAG in which text chunks are retrieved and added to the\navailable context window until the specified token limit is reached.\nThe size of the context window and the prompts used for answer generation are the same across\nall six conditions (except for minor modifications to reference styles to match the types of context\ninformation used). Conditions only differ in how the contents of the context window are created.\nThe graph index supporting conditions C0-C3 was created using our generic prompts for entity and\nrelationship extraction only, with entity types and few-shot examples tailored to the domain of the\ndata. The graph indexing process used a context window size of 600 tokens with 1 gleaning for the\nPodcast dataset and 0 gleanings for the News dataset.\n3.4 Metrics\nLLMs have been shown to be good evaluators of natural language generation, achieving state-of-\nthe-art or competitive results compared against human judgements (Wang et al., 2023a; Zheng et al.,\n2024). While this approach can generate reference-based metrics when gold standard answers are\nknown, it is also capable of measuring the qualities of generated texts (e.g., fluency) in a reference-\nfree style (Wang et al., 2023a) as well as in head-to-head comparison of competing outputs (LLM-\nas-a-judge, Zheng et al., 2024). LLMs have also shown promise at evaluating the performance of\nconventional RAG systems, automatically evaluating qualities like context relevance, faithfulness,\nand answer relevance (RAGAS, Es et al., 2023).\nGiven the multi-stage nature of our Graph RAG mechanism, the multiple conditions we wanted to\ncompare, and the lack of gold standard answers to our activity-based sensemaking questions, we\ndecided to adopt a head-to-head comparison approach using an LLM evaluator. We selected three\ntarget metrics capturing qualities that are desirable for sensemaking activities, as well as a control\nmetric (directness) used as a indicator of validity. Since directness is effectively in opposition to\ncomprehensiveness and diversity, we would not expect any method to win across all four metrics.\nOur head-to-head measures computed using an LLM evaluator are as follows:\n• Comprehensiveness. How much detail does the answer provide to cover all aspects and\ndetails of the question?\n• Diversity. How varied and rich is the answer in providing different perspectives and insights\non the question?\n• Empowerment. How well does the answer help the reader understand and make informed\njudgements about the topic?\n• Directness. How specifically and clearly does the answer address the question?\nFor our evaluation, the LLM is provided with the question, target metric, and a pair of answers, and\nasked to assess which answer is better according to the metric, as well as why. It returns the winner\nif one exists, otherwise a tie if they are fundamentally similar and the differences are negligible.\nTo account for the stochasticity of LLMs, we run each comparison five times and use mean scores.\nTable 2 shows an example of LLM-generated assessment.\n7\nQuestion Which public figures are repeatedly mentioned across various entertainment articles?\nGraph\nRAG\nOverview of Prominent Public Figures in Entertainment\nThe entertainment industry is vast and diverse, encompassing film, television, music, sports, and\ndigital media. Certain public figures stand out due to their significant contributions and influence\nacross these sectors. The following summary highlights key individuals who are repeatedly\nmentioned in various entertainment articles, reflecting their impact and presence within the industry.\nActors and Directors [...] Public Figures in Controversy [...] Musicians and Executives [...]\nAthletes and Coaches [...] Influencers and Entrepreneurs [...]\nThe repeated mention of these figures in entertainment articles signifies their ongoing relevance and\nthe public’s interest in their work. Their influence spans across various aspects of entertainment, from\nshaping cultural narratives in film and television to driving trends in music and digital media. These\nindividuals not only contribute to their respective fields but also influence the broader cultural\nlandscape, often becoming central figures in social discussions and public discourse.\nNa",
    "chunk_order_index": 4,
    "full_doc_id": "doc-2967b55fb6e8db0d5c634c9f010180a4"
  },
  "chunk-bd4ef716955e38ba1411e717dacdd630": {
    "tokens": 1200,
    "content": "Musicians and Executives [...]\nAthletes and Coaches [...] Influencers and Entrepreneurs [...]\nThe repeated mention of these figures in entertainment articles signifies their ongoing relevance and\nthe public’s interest in their work. Their influence spans across various aspects of entertainment, from\nshaping cultural narratives in film and television to driving trends in music and digital media. These\nindividuals not only contribute to their respective fields but also influence the broader cultural\nlandscape, often becoming central figures in social discussions and public discourse.\nNa¨ıve\nRAG\nPublic figures who are repeatedly mentioned across various entertainment articles include Taylor\nSwift, Travis Kelce, Britney Spears, and Justin Timberlake. These individuals have been highlighted\nfor various reasons, ranging from their professional achievements to their personal lives.\nTaylor Swift [...] Travis Kelce [...] Britney Spears [...] Justin Timberlake [...]\nThese figures are frequently covered due to their high-profile status and the public’s interest in their\ncareers and personal lives. Their activities, whether in music, sports, or personal relationships, have\nsignificant cultural and economic impacts, as evidenced by the media coverage and public reactions.\nLLM\nDecision\nComprehensiveness: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a more comprehensive and detailed list of public figures from a\nwider range of entertainment sectors, including film, television, music, sports, gaming, and digital\nmedia. It also includes specific examples of their contributions and the impact they have on their\nrespective fields, as well as mentions of controversies and their implications. Answer 2, while\ndetailed in its coverage of a few individuals, is limited to a smaller number of public figures and\nfocuses primarily on their personal lives and relationships rather than a broad spectrum of their\nprofessional influence across the entertainment industry.\nDiversity: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a more varied and rich response by covering a wide range of\npublic figures from different sectors of the entertainment industry, including film, television, music,\nsports, gaming, and digital media. It offers insights into the contributions and influence of these\nfigures, as well as controversies and their impact on public discourse. The answer also cites specific\ndata sources for each mentioned figure, indicating a diverse range of evidence to support the claims.\nIn contrast, Answer 2 focuses on a smaller group of public figures, primarily from the music industry\nand sports, and relies heavily on a single source for data, which makes it less diverse in perspectives\nand insights.\nEmpowerment: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a comprehensive and structured overview of public figures\nacross various sectors of the entertainment industry, including film, television, music, sports, and\ndigital media. It lists multiple individuals, providing specific examples of their contributions and the\ncontext in which they are mentioned in entertainment articles, along with references to data reports\nfor each claim. This approach helps the reader understand the breadth of the topic and make informed\njudgments without being misled. In contrast, Answer 2 focuses on a smaller group of public figures\nand primarily discusses their personal lives and relationships, which may not provide as broad an\nunderstanding of the topic. While Answer 2 also cites sources, it does not match the depth and variety\nof Answer 1.\nDirectness: Winner=2 (Na¨ıve RAG)\nAnswer 2 is better because it directly lists specific public figures who are repeatedly mentioned\nacross various entertainment articles, such as Taylor Swift, Travis Kelce, Britney Spears, and Justin\nTimberlake, and provides concise explanations for their frequent mentions. Answer 1, while\ncomprehensive, includes a lot of detailed information about various figures in different sectors of\nentertainment, which, while informative, does not directly answer the question with the same level of\nconciseness and specificity as Answer 2.\nTable 2: Example question for the News article dataset, with generated answers from Graph RAG\n(C2) and Na¨ıve RAG, as well as LLM-generated assessments.\n8\nPodcast transcripts\n50 17 28 25 22 21\n83 50 50 48 43 44\n72 50 50 53 50 49\n75 52 47 50 52 50\n78 57 50 48 50 52\n79 56 51 50 48 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nComprehensiveness\n50 18 23 25 19 19\n82 50 50 50 43 46\n77 50 50 50 46 44\n75 50 50 50 44 45\n81 57 54 56 50 48\n81 54 56 55 52 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nDiversity\n50 42 57 52 49 51\n58 50 59 55 52 51\n43 41 50 49 47 48\n48 45 51 50 49 50\n51 48 53 51 50 51\n49 49 52 50 49 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nEmpowerment\n50 56 65 60 60 60\n44 50 55 52 51 52\n35 45 50 47 48 48\n40 48 53",
    "chunk_order_index": 5,
    "full_doc_id": "doc-2967b55fb6e8db0d5c634c9f010180a4"
  },
  "chunk-cdd66960b4484728cb0781c54f087cd1": {
    "tokens": 1200,
    "content": "49 50\n51 48 53 51 50 51\n49 49 52 50 49 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nEmpowerment\n50 56 65 60 60 60\n44 50 55 52 51 52\n35 45 50 47 48 48\n40 48 53 50 50 50\n40 49 52 50 50 50\n40 48 52 50 50 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nDirectness\nNews articles\n50 20 28 25 21 21\n80 50 44 41 38 36\n72 56 50 52 54 52\n75 59 48 50 58 55\n79 62 46 42 50 59\n79 64 48 45 41 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nComprehensiveness\n50 33 38 35 29 31\n67 50 53 45 44 40\n62 47 50 40 41 41\n65 55 60 50 50 50\n71 56 59 50 50 51\n69 60 59 50 49 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nDiversity\n50 47 57 49 50 50\n53 50 58 50 50 48\n43 42 50 42 45 44\n51 50 58 50 52 51\n50 50 55 48 50 50\n50 52 56 49 50 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nEmpowerment\n50 54 59 55 55 54\n46 50 55 53 52 52\n41 45 50 48 48 47\n45 47 52 50 49 49\n45 48 52 51 50 49\n46 48 53 51 51 50\nSS\nTS\nC0\nC1\nC2\nC3\nSS TS C0 C1 C2 C3\nDirectness\nFigure 4: Head-to-head win rate percentages of (row condition) over (column condition) across two\ndatasets, four metrics, and 125 questions per comparison (each repeated five times and averaged).\nThe overall winner per dataset and metric is shown in bold. Self-win rates were not computed but\nare shown as the expected 50% for reference. All Graph RAG conditions outperformed na¨ıve RAG\non comprehensiveness and diversity. Conditions C1-C3 also showed slight improvements in answer\ncomprehensiveness and diversity over TS (global text summarization without a graph index).\n3.5 Configuration\nThe effect of context window size on any particular task is unclear, especially for models like\ngpt-4-turbo with a large context size of 128k tokens. Given the potential for information to\nbe “lost in the middle” of longer contexts (Kuratov et al., 2024; Liu et al., 2023), we wanted to ex-\nplore the effects of varying the context window size for our combinations of datasets, questions, and\nmetrics. In particular, our goal was to determine the optimum context size for our baseline condition\n(SS) and then use this uniformly for all query-time LLM use. To that end, we tested four context\nwindow sizes: 8k, 16k, 32k and 64k. Surprisingly, the smallest context window size tested (8k)\nwas universally better for all comparisons on comprehensiveness (average win rate of 58.1%), while\nperforming comparably with larger context sizes on diversity (average win rate = 52.4%), and em-\npowerment (average win rate = 51.3%). Given our preference for more comprehensive and diverse\nanswers, we therefore used a fixed context window size of 8k tokens for the final evaluation.\n3.6 Results\nThe indexing process resulted in a graph consisting of 8564 nodes and 20691 edges for the Podcast\ndataset, and a larger graph of 15754 nodes and 19520 edges for the News dataset. Table 3 shows the\nnumber of community summaries at different levels of each graph community hierarchy.\nGlobal approaches vs. na ¨ıve RAG. As shown in Figure 4, global approaches consistently out-\nperformed the na ¨ıve RAG (SS) approach in both comprehensiveness and diversity metrics across\ndatasets. Specifically, global approaches achieved comprehensiveness win rates between 72-83%\nfor Podcast transcripts and 72-80% for News articles, while diversity win rates ranged from 75-82%\nand 62-71% respectively. Our use of directness as a validity test also achieved the expected results,\ni.e., that na¨ıve RAG produces the most direct responses across all comparisons.\n9\nPodcast Transcripts News Articles\nC0 C1 C2 C3 TS C0 C1 C2 C3 TS\nUnits 34 367 969 1310 1669 55",
    "chunk_order_index": 6,
    "full_doc_id": "doc-2967b55fb6e8db0d5c634c9f010180a4"
  },
  "chunk-8bc94ade99820b3913b80f0e77fbb5ec": {
    "tokens": 1200,
    "content": "% for News articles, while diversity win rates ranged from 75-82%\nand 62-71% respectively. Our use of directness as a validity test also achieved the expected results,\ni.e., that na¨ıve RAG produces the most direct responses across all comparisons.\n9\nPodcast Transcripts News Articles\nC0 C1 C2 C3 TS C0 C1 C2 C3 TS\nUnits 34 367 969 1310 1669 55 555 1797 2142 3197\nTokens 26657 225756 565720 746100 1014611 39770 352641 980898 1140266 1707694\n% Max 2.6 22.2 55.8 73.5 100 2.3 20.7 57.4 66.8 100\nTable 3: Number of context units (community summaries forC0-C3 and text chunks for TS), corre-\nsponding token counts, and percentage of the maximum token count. Map-reduce summarization of\nsource texts is the most resource-intensive approach requiring the highest number of context tokens.\nRoot-level community summaries (C0) require dramatically fewer tokens per query (9x-43x).\nCommunity summaries vs. source texts. When comparing community summaries to source texts\nusing Graph RAG, community summaries generally provided a small but consistent improvement\nin answer comprehensiveness and diversity, except for root-level summaries. Intermediate-level\nsummaries in the Podcast dataset and low-level community summaries in the News dataset achieved\ncomprehensiveness win rates of 57% and 64%, respectively. Diversity win rates were 57% for\nPodcast intermediate-level summaries and 60% for News low-level community summaries. Table 3\nalso illustrates the scalability advantages of Graph RAG compared to source text summarization: for\nlow-level community summaries ( C3), Graph RAG required 26-33% fewer context tokens, while\nfor root-level community summaries (C0), it required over 97% fewer tokens. For a modest drop in\nperformance compared with other global methods, root-level Graph RAG offers a highly efficient\nmethod for the iterative question answering that characterizes sensemaking activity, while retaining\nadvantages in comprehensiveness (72% win rate) and diversity (62% win rate) over na¨ıve RAG.\nEmpowerment. Empowerment comparisons showed mixed results for both global approaches versus\nna¨ıve RAG (SS) and Graph RAG approaches versus source text summarization (TS). Ad-hoc LLM\nuse to analyze LLM reasoning for this measure indicated that the ability to provide specific exam-\nples, quotes, and citations was judged to be key to helping users reach an informed understanding.\nTuning element extraction prompts may help to retain more of these details in the Graph RAG index.\n4 Related Work\n4.1 RAG Approaches and Systems\nWhen using LLMs, RAG involves first retrieving relevant information from external data sources,\nthen adding this information to the context window of the LLM along with the original query (Ram\net al., 2023). Na ¨ıve RAG approaches (Gao et al., 2023) do this by converting documents to text,\nsplitting text into chunks, and embedding these chunks into a vector space in which similar positions\nrepresent similar semantics. Queries are then embedded into the same vector space, with the text\nchunks of the nearest k vectors used as context. More advanced variations exist, but all solve the\nproblem of what to do when an external dataset of interest exceeds the LLM’s context window.\nAdvanced RAG systems include pre-retrieval, retrieval, post-retrieval strategies designed to over-\ncome the drawbacks of Na¨ıve RAG, while Modular RAG systems include patterns for iterative and\ndynamic cycles of interleaved retrieval and generation (Gao et al., 2023). Our implementation of\nGraph RAG incorporates multiple concepts related to other systems. For example, our community\nsummaries are a kind of self-memory (Selfmem, Cheng et al., 2024) for generation-augmented re-\ntrieval (GAR, Mao et al., 2020) that facilitates future generation cycles, while our parallel generation\nof community answers from these summaries is a kind of iterative (Iter-RetGen, Shao et al., 2023)\nor federated (FeB4RAG, Wang et al., 2024) retrieval-generation strategy. Other systems have also\ncombined these concepts for multi-document summarization (CAiRE-COVID, Su et al., 2020) and\nmulti-hop question answering (ITRG, Feng et al., 2023; IR-CoT, Trivedi et al., 2022; DSP, Khattab\net al., 2022). Our use of a hierarchical index and summarization also bears resemblance to further\napproaches, such as generating a hierarchical index of text chunks by clustering the vectors of text\nembeddings (RAPTOR, Sarthi et al., 2024) or generating a “tree of clarifications” to answer mul-\ntiple interpretations of ambiguous questions (Kim et al., 2023). However, none of these iterative or\nhierarchical approaches use the kind of self-generated graph index that enables Graph RAG.\n10\n4.2 Graphs and LLMs\nUse of graphs in connection with LLMs and RAG is a developing research area, with multiple\ndirections already established. These include using LLMs for knowledge graph creation (Tra-\njanoska et al., 2023) and completion (Yao et al., 2023), as well as for the",
    "chunk_order_index": 7,
    "full_doc_id": "doc-2967b55fb6e8db0d5c634c9f010180a4"
  },
  "chunk-4f652f8c2415208002c35fb88e8a4430": {
    "tokens": 1200,
    "content": "of these iterative or\nhierarchical approaches use the kind of self-generated graph index that enables Graph RAG.\n10\n4.2 Graphs and LLMs\nUse of graphs in connection with LLMs and RAG is a developing research area, with multiple\ndirections already established. These include using LLMs for knowledge graph creation (Tra-\njanoska et al., 2023) and completion (Yao et al., 2023), as well as for the extraction of causal\ngraphs (Ban et al., 2023; Zhang et al., 2024) from source texts. They also include forms of ad-\nvanced RAG (Gao et al., 2023) where the index is a knowledge graph (KAPING, Baek et al., 2023),\nwhere subsets of the graph structure (G-Retriever, He et al., 2024) or derived graph metrics (Graph-\nToolFormer, Zhang, 2023) are the objects of enquiry, where narrative outputs are strongly grounded\nin the facts of retrieved subgraphs (SURGE, Kang et al., 2023), where retrieved event-plot sub-\ngraphs are serialized using narrative templates (FABULA, Ranade and Joshi, 2023), and where the\nsystem supports both creation and traversal of text-relationship graphs for multi-hop question an-\nswering (Wang et al., 2023b). In terms of open-source software, a variety a graph databases are\nsupported by both the LangChain (LangChain, 2024) and LlamaIndex (LlamaIndex, 2024) libraries,\nwhile a more general class of graph-based RAG applications is also emerging, including systems that\ncan create and reason over knowledge graphs in both Neo4J (NaLLM, Neo4J, 2024) and Nebula-\nGraph (GraphRAG, NebulaGraph, 2024) formats. Unlike our Graph RAG approach, however, none\nof these systems use the natural modularity of graphs to partition data for global summarization.\n5 Discussion\nLimitations of evaluation approach . Our evaluation to date has only examined a certain class of\nsensemaking questions for two corpora in the region of 1 million tokens. More work is needed\nto understand how performance varies across different ranges of question types, data types, and\ndataset sizes, as well as to validate our sensemaking questions and target metrics with end users.\nComparison of fabrication rates, e.g., using approaches like SelfCheckGPT (Manakul et al., 2023),\nwould also improve on the current analysis.\nTrade-offs of building a graph index. We consistently observed Graph RAG achieve the best head-\nto-head results against other methods, but in many cases the graph-free approach to global summa-\nrization of source texts performed competitively. The real-world decision about whether to invest in\nbuilding a graph index depends on multiple factors, including the compute budget, expected number\nof lifetime queries per dataset, and value obtained from other aspects of the graph index (including\nthe generic community summaries and the use of other graph-related RAG approaches).\nFuture work. The graph index, rich text annotations, and hierarchical community structure support-\ning the current Graph RAG approach offer many possibilities for refinement and adaptation. This\nincludes RAG approaches that operate in a more local manner, via embedding-based matching of\nuser queries and graph annotations, as well as the possibility of hybrid RAG schemes that combine\nembedding-based matching against community reports before employing our map-reduce summa-\nrization mechanisms. This “roll-up” operation could also be extended across more levels of the\ncommunity hierarchy, as well as implemented as a more exploratory “drill down” mechanism that\nfollows the information scent contained in higher-level community summaries.\n6 Conclusion\nWe have presented a global approach to Graph RAG, combining knowledge graph generation,\nretrieval-augmented generation (RAG), and query-focused summarization (QFS) to support human\nsensemaking over entire text corpora. Initial evaluations show substantial improvements over a\nna¨ıve RAG baseline for both the comprehensiveness and diversity of answers, as well as favorable\ncomparisons to a global but graph-free approach using map-reduce source text summarization. For\nsituations requiring many global queries over the same dataset, summaries of root-level communi-\nties in the entity-based graph index provide a data index that is both superior to na ¨ıve RAG and\nachieves competitive performance to other global methods at a fraction of the token cost.\nAn open-source, Python-based implementation of both global and local Graph RAG approaches is\nforthcoming at https://aka.ms/graphrag.\n11\nAcknowledgements\nWe would also like to thank the following people who contributed to the work: Alonso Guevara\nFern´andez, Amber Hoak, Andr ´es Morales Esquivel, Ben Cutler, Billie Rinaldi, Chris Sanchez,\nChris Trevino, Christine Caggiano, David Tittsworth, Dayenne de Souza, Douglas Orbaker, Ed\nClark, Gabriel Nieves-Ponce, Gaudy Blanco Meneses, Kate Lytvynets, Katy Smith, M´onica Carva-\njal, Nathan Evans, Richard Ortega, Rodrigo Racanicci, Sarah Smith, and Shane Solomon.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Al-\ntenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nAnil, R., Borgeaud, S., Wu",
    "chunk_order_index": 8,
    "full_doc_id": "doc-2967b55fb6e8db0d5c634c9f010180a4"
  },
  "chunk-77e8b7478a3ebaeb256aefc827351455": {
    "tokens": 1200,
    "content": "and Shane Solomon.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Al-\ntenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nAnil, R., Borgeaud, S., Wu, Y ., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M.,\nHauth, A., et al. (2023). Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805.\nBaek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for\nzero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136.\nBan, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing\nlarge language models for advanced causal discovery from data.\nBaumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorpo-\nrating query relevance, multi-document coverage, and summary length constraints into seq2seq\nmodels. arXiv preprint arXiv:1801.07704.\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\n2008(10):P10008.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in\nneural information processing systems, 33:1877–1901.\nCheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., and Yan, R. (2024). Lift yourself up: Retrieval-\naugmented text generation with self-memory. Advances in Neural Information Processing Sys-\ntems, 36.\nDang, H. T. (2006). Duc 2005: Evaluation of question-focused summarization systems. InProceed-\nings of the Workshop on Task-Focused Summarization and Question Answering, pages 48–55.\nEs, S., James, J., Espinosa-Anke, L., and Schockaert, S. (2023). Ragas: Automated evaluation of\nretrieval augmented generation. arXiv preprint arXiv:2309.15217.\nFeng, Z., Feng, X., Zhao, D., Yang, M., and Qin, B. (2023). Retrieval-generation synergy augmented\nlarge language models. arXiv preprint arXiv:2310.05149.\nFortunato, S. (2010). Community detection in graphs. Physics reports, 486(3-5):75–174.\nGao, Y ., Xiong, Y ., Gao, X., Jia, K., Pan, J., Bi, Y ., Dai, Y ., Sun, J., and Wang, H. (2023). Retrieval-\naugmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997.\nGoodwin, T. R., Savery, M. E., and Demner-Fushman, D. (2020). Flight of the pegasus? comparing\ntransformers on few-shot and zero-shot multi-document abstractive summarization. In Proceed-\nings of COLING. International Conference on Computational Linguistics , volume 2020, page\n5640. NIH Public Access.\nHe, X., Tian, Y ., Sun, Y ., Chawla, N. V ., Laurent, T., LeCun, Y ., Bresson, X., and Hooi, B. (2024).\nG-retriever: Retrieval-augmented generation for textual graph understanding and question an-\nswering. arXiv preprint arXiv:2402.07630.\n12\nJacomy, M., Venturini, T., Heymann, S., and Bastian, M. (2014). Forceatlas2, a continuous graph\nlayout algorithm for handy network visualization designed for the gephi software. PLoS ONE\n9(6): e98679. https://doi.org/10.1371/journal.pone.0098679.\nJin, D., Yu, Z., Jiao, P., Pan, S., He, D., Wu, J., Philip, S. Y ., and Zhang, W. (2021). A survey of\ncommunity detection approaches: From statistical modeling to deep learning. IEEE Transactions\non Knowledge and Data Engineering, 35(2):1149–1170.\nKang, M., Kwak, J. M., Baek, J., and Hwang, S. J. (2023). Knowledge graph-augmented language\nmodels for knowledge-grounded dialogue generation. arXiv preprint arXiv:2305.18846.\nKhattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M.",
    "chunk_order_index": 9,
    "full_doc_id": "doc-2967b55fb6e8db0d5c634c9f010180a4"
  },
  "chunk-469d6c0b9916605fb460ed90b1252d7a": {
    "tokens": 1200,
    "content": "–1170.\nKang, M., Kwak, J. M., Baek, J., and Hwang, S. J. (2023). Knowledge graph-augmented language\nmodels for knowledge-grounded dialogue generation. arXiv preprint arXiv:2305.18846.\nKhattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. (2022).\nDemonstrate-search-predict: Composing retrieval and language models for knowledge-intensive\nnlp. arXiv preprint arXiv:2212.14024.\nKim, G., Kim, S., Jeon, B., Park, J., and Kang, J. (2023). Tree of clarifications: Answering ambigu-\nous questions with retrieval-augmented large language models.arXiv preprint arXiv:2310.14696.\nKlein, G., Moon, B., and Hoffman, R. R. (2006a). Making sense of sensemaking 1: Alternative\nperspectives. IEEE intelligent systems, 21(4):70–73.\nKlein, G., Moon, B., and Hoffman, R. R. (2006b). Making sense of sensemaking 2: A macrocogni-\ntive model. IEEE Intelligent systems, 21(5):88–92.\nKoesten, L., Gregory, K., Groth, P., and Simperl, E. (2021). Talking datasets–understanding data\nsensemaking behaviours. International journal of human-computer studies, 146:102562.\nKuratov, Y ., Bulatov, A., Anokhin, P., Sorokin, D., Sorokin, A., and Burtsev, M. (2024). In search\nof needles in a 11m haystack: Recurrent memory finds what llms miss.\nLangChain (2024). Langchain graphs. https://python .langchain.com/docs/use cases/graph/.\nLaskar, M. T. R., Hoque, E., and Huang, J. (2020). Query focused abstractive summarization via\nincorporating query relevance and transfer learning with transformer models. In Advances in\nArtificial Intelligence: 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020,\nOttawa, ON, Canada, May 13–15, 2020, Proceedings 33, pages 342–348. Springer.\nLaskar, M. T. R., Hoque, E., and Huang, J. X. (2022). Domain adaptation with pre-trained transform-\ners for query-focused abstractive text summarization. Computational Linguistics, 48(2):279–320.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., K ¨uttler, H., Lewis, M., Yih,\nW.-t., Rockt¨aschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp\ntasks. Advances in Neural Information Processing Systems, 33:9459–9474.\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. (2023). Lost\nin the middle: How language models use long contexts. arXiv:2307.03172.\nLiu, Y . and Lapata, M. (2019). Hierarchical transformers for multi-document summarization.arXiv\npreprint arXiv:1905.13164.\nLlamaIndex (2024). LlamaIndex Knowledge Graph Index. https://docs .llamaindex.ai/en/stable/\nexamples/index structs/knowledge graph/KnowledgeGraphDemo.html.\nManakul, P., Liusie, A., and Gales, M. J. (2023). Selfcheckgpt: Zero-resource black-box hallucina-\ntion detection for generative large language models. arXiv preprint arXiv:2303.08896.\nMao, Y ., He, P., Liu, X., Shen, Y ., Gao, J., Han, J., and Chen, W. (2020). Generation-augmented\nretrieval for open-domain question answering. arXiv preprint arXiv:2009.08553.\nMartin, S., Brown, W. M., Klavans, R., and Boyack, K. (2011). Openord: An open-source toolbox\nfor large graph layout. SPIE Conference on Visualization and Data Analysis (VDA).\nMicrosoft (2023). The impact of large language models on scientific discovery: a preliminary study\nusing gpt-4.\n13\nNebulaGraph (2024). Nebulagraph launches industry-first graph rag: Retrieval-augmented genera-\ntion with llm based on knowledge graphs. https://www.nebula-graph.io/posts/graph-RAG.\nNeo4J (2024). Project NaLLM. https://github .com/neo4j/NaLLM.\nNewman, M. E. (2006). Modularity and community structure in networks. Proceedings of the\nnational academy of sciences, 103(23):8577–8582.\nRam, O., Levine, Y ., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham,\nY . (2023). In-context retrieval-augmented language models. Transactions of the Association for\nComputational Linguistics, 11:1316–1331.",
    "chunk_order_index": 10,
    "full_doc_id": "doc-2967b55fb6e8db0d5c634c9f010180a4"
  },
  "chunk-a48f0f44b342a20fd12974169617802b": {
    "tokens": 1200,
    "content": "Modularity and community structure in networks. Proceedings of the\nnational academy of sciences, 103(23):8577–8582.\nRam, O., Levine, Y ., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham,\nY . (2023). In-context retrieval-augmented language models. Transactions of the Association for\nComputational Linguistics, 11:1316–1331.\nRanade, P. and Joshi, A. (2023). Fabula: Intelligence report generation using retrieval-augmented\nnarrative construction. arXiv preprint arXiv:2310.13848.\nSarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., and Manning, C. D. (2024). Raptor:\nRecursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059.\nScott, K. (2024). Behind the Tech. https://www .microsoft.com/en-us/behind-the-tech.\nShao, Z., Gong, Y ., Shen, Y ., Huang, M., Duan, N., and Chen, W. (2023). Enhancing retrieval-\naugmented large language models with iterative retrieval-generation synergy. arXiv preprint\narXiv:2305.15294.\nSu, D., Xu, Y ., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A ques-\ntion answering and query-focused multi-document summarization system for covid-19 scholarly\ninformation management. arXiv preprint arXiv:2005.03975.\nTang, Y . and Yang, Y . (2024). MultiHop-RAG: Benchmarking retrieval-augmented generation for\nmulti-hop queries. arXiv preprint arXiv:2401.15391.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,\nBhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288.\nTraag, V . A., Waltman, L., and Van Eck, N. J. (2019). From Louvain to Leiden: guaranteeing\nwell-connected communities. Scientific Reports, 9(1).\nTrajanoska, M., Stojanov, R., and Trajanov, D. (2023). Enhancing knowledge graph construction\nusing large language models. ArXiv, abs/2305.04676.\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. (2022). Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint\narXiv:2212.10509.\nWang, J., Liang, Y ., Meng, F., Sun, Z., Shi, H., Li, Z., Xu, J., Qu, J., and Zhou, J. (2023a). Is chatgpt\na good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048.\nWang, S., Khramtsova, E., Zhuang, S., and Zuccon, G. (2024). Feb4rag: Evaluating federated search\nin the context of retrieval augmented generation. arXiv preprint arXiv:2402.11891.\nWang, Y ., Lipka, N., Rossi, R. A., Siu, A., Zhang, R., and Derr, T. (2023b). Knowledge graph\nprompting for multi-document question answering.\nXu, Y . and Lapata, M. (2021). Text summarization with latent queries. arXiv preprint\narXiv:2106.00104.\nYang, Z., Qi, P., Zhang, S., Bengio, Y ., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. (2018).\nHotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on\nEmpirical Methods in Natural Language Processing (EMNLP).\nYao, J.-g., Wan, X., and Xiao, J. (2017). Recent advances in document summarization. Knowledge\nand Information Systems, 53:297–336.\n14\nYao, L., Peng, J., Mao, C., and Luo, Y . (2023). Exploring large language models for knowledge\ngraph completion.\nZhang, J. (2023). Graph-toolformer: To empower llms with graph reasoning ability via prompt\naugmented by chatgpt. arXiv preprint arXiv:2304.11116.\nZhang, Y ., Zhang, Y ., Gan, Y ., Yao, L., and Wang, C. (2024). Causal graph discovery with retrieval-\naugmented generation based large language models. arXiv preprint arXiv:2402.15301.\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,\nE., et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot",
    "chunk_order_index": 11,
    "full_doc_id": "doc-2967b55fb6e8db0d5c634c9f010180a4"
  },
  "chunk-916f35d0705c09db97e5283af8e72e84": {
    "tokens": 114,
    "content": "(2024). Causal graph discovery with retrieval-\naugmented generation based large language models. arXiv preprint arXiv:2402.15301.\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,\nE., et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\nInformation Processing Systems, 36.\n15",
    "chunk_order_index": 12,
    "full_doc_id": "doc-2967b55fb6e8db0d5c634c9f010180a4"
  },
  "chunk-cafdfbe69a88ca01d85f442228a39254": {
    "tokens": 1200,
    "content": "LIGHT RAG: S IMPLE AND FAST\nRETRIEVAL -AUGMENTED GENERATION\nZirui Guo1,2, Lianghao Xia2, Yanhua Yu1, Tu Ao1, Chao Huang2∗\nBeijing University of Posts and Telecommunications1\nUniversity of Hong Kong2\nzrguo101@hku.hk aka_xia@foxmail.com chaohuang75@gmail.com\nABSTRACT\nRetrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data representa-\ntions and inadequate contextual awareness, which can lead to fragmented answers\nthat fail to capture complex inter-dependencies. To address these challenges, we\npropose LightRAG, which incorporates graph structures into text indexing and\nretrieval processes. This innovative framework employs a dual-level retrieval sys-\ntem that enhances comprehensive information retrieval from both low-level and\nhigh-level knowledge discovery. Additionally, the integration of graph structures\nwith vector representations facilitates efficient retrieval of related entities and their\nrelationships, significantly improving response times while maintaining contextual\nrelevance. This capability is further enhanced by an incremental update algorithm\nthat ensures the timely integration of new data, allowing the system to remain\neffective and responsive in rapidly changing data environments. Extensive experi-\nmental validation demonstrates considerable improvements in retrieval accuracy\nand efficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG.\n1 I NTRODUCTION\nRetrieval-Augmented Generation (RAG) systems have been developed to enhance large language\nmodels (LLMs) by integrating external knowledge sources Sudhi et al. (2024); Es et al. (2024);\nSalemi & Zamani (2024). This innovative integration allows LLMs to generate more accurate and\ncontextually relevant responses, significantly improving their utility in real-world applications. By\nadapting to specific domain knowledge Tu et al. (2024), RAG systems ensure that the information\nprovided is not only pertinent but also tailored to the user’s needs. Furthermore, they offer access to\nup-to-date information Zhao et al. (2024), which is crucial in rapidly evolving fields. Chunking plays\na vital role in facilitating the retrieval-augmented generation process Lyu et al. (2024). By breaking\ndown a large external text corpus into smaller, more manageable segments, chunking significantly\nenhances the accuracy of information retrieval. This approach allows for more targeted similarity\nsearches, ensuring that the retrieved content is directly relevant to user queries.\nHowever, existing RAG systems have key limitations that hinder their performance. First, many\nmethods rely on flat data representations, restricting their ability to understand and retrieve information\nbased on intricate relationships between entities. Second, these systems often lack the contextual\nawareness needed to maintain coherence across various entities and their interrelations, resulting\nin responses that may not fully address user queries. For example, consider a user asking, “How\ndoes the rise of electric vehicles influence urban air quality and public transportation infrastructure?”\nExisting RAG methods might retrieve separate documents on electric vehicles, air pollution, and\npublic transportation challenges but struggle to synthesize this information into a cohesive response.\nThey may fail to explain how the adoption of electric vehicles can improve air quality, which in turn\n∗Chao Huang is the corresponding author.\n1\narXiv:2410.05779v2  [cs.IR]  7 Nov 2024\ncould affect public transportation planning. As a result, the user may receive a fragmented answer\nthat does not adequately capture the complex inter-dependencies among these topics.\nTo address these limitations, we propose incorporating graph structures into text indexing and\nrelevant information retrieval. Graphs are particularly effective at representing the interdependencies\namong different entities Rampášek et al. (2022), which enables a more nuanced understanding\nof relationships. The integration of graph-based knowledge structures facilitates the synthesis of\ninformation from multiple sources into coherent and contextually rich responses. Despite these\nadvantages, developing a fast and scalable graph-empowered RAG system that efficiently handles\nvarying query volumes is crucial. In this work, we achieve an effective and efficient RAG system by\naddressing three key challenges: i) Comprehensive Information Retrieval. Ensuring comprehensive\ninformation retrieval that captures the full context of inter-dependent entities from all documents;\nii) Enhanced Retrieval Efficiency. Improving retrieval efficiency over the graph-based knowledge\nstructures to significantly reduce response times; iii) Rapid Adaptation to New Data. Enabling\nquick adaptation to new data updates, ensuring the system remains relevant in dynamic environments.\nIn response to the outlined challenges, we propose LightRAG, a model that seamlessly integrates a\ngraph-based text indexing paradigm with a dual-level retrieval framework. This innovative approach\nenhances the system’s capacity to capture complex inter-dependencies among entities, resulting in\nmore coherent and contextually rich responses. LightRAG employs efficient dual-level retrieval\nstrategies: low-level retrieval, which focuses on precise information about specific entities and their\nrelationships, and high-level retrieval, which encompasses broader topics and themes. By combining\nboth detailed and conceptual retrieval, LightRAG effectively accommodates a diverse range of quries,\nensuring that users receive relevant and comprehensive responses tailored to their specific needs.\nAdditionally, by integrating graph structures with vector representations, our framework facilitates\nefficient retrieval of related entities and relations while enhancing the comprehensiveness of results\nthrough relevant structural information from the constructed knowledge graph.\nIn summary, the key contributions of this work are highlighted as follows:\n• General Aspect. We emphasize the importance of developing a graph",
    "chunk_order_index": 0,
    "full_doc_id": "doc-45052d5e5c880b8ad46be8afde8266c9"
  },
  "chunk-e6debf563390e9a07a3d588f65a9fdc4": {
    "tokens": 1200,
    "content": "both detailed and conceptual retrieval, LightRAG effectively accommodates a diverse range of quries,\nensuring that users receive relevant and comprehensive responses tailored to their specific needs.\nAdditionally, by integrating graph structures with vector representations, our framework facilitates\nefficient retrieval of related entities and relations while enhancing the comprehensiveness of results\nthrough relevant structural information from the constructed knowledge graph.\nIn summary, the key contributions of this work are highlighted as follows:\n• General Aspect. We emphasize the importance of developing a graph-empowered RAG system to\novercome the limitations of existing methods. By integrating graph structures into text indexing,\nwe can effectively represent complex interdependencies among entities, fostering a nuanced\nunderstanding of relationships and enabling coherent, contextually rich responses.\n• Methodologies. To enable an efficient and adaptive RAG system, we propose LightRAG, which\nintegrates a dual-level retrieval paradigm with graph-enhanced text indexing. This approach\ncaptures both low-level and high-level information for comprehensive, cost-effective retrieval.\nBy eliminating the need to rebuild the entire index, LightRAG reduces computational costs and\naccelerates adaptation, while its incremental update algorithm ensures timely integration of new\ndata, maintaining effectiveness in dynamic environments.\n• Experimental Findings. Extensive experiments were conducted to evaluate the effectiveness of\nLightRAG in comparison to existing RAG models. These assessments focused on several key\ndimensions, including retrieval accuracy, model ablation, response efficiency, and adaptability to\nnew information. The results demonstrated significant improvements over baseline methods.\n2 R ETRIEVAL -AUGMENTED GENERATION\nRetrieval-Augmented Generation (RAG) integrates user queries with a collection of pertinent doc-\numents sourced from an external knowledge database, incorporating two essential elements: the\nRetrieval Component and the Generation Component. 1) The retrieval component is responsible\nfor fetching relevant documents or information from the external knowledge database. It identifies and\nretrieves the most pertinent data based on the input query. 2) After the retrieval process, the generation\ncomponent takes the retrieved information and generates coherent, contextually relevant responses. It\nleverages the capabilities of the language model to produce meaningful outputs. Formally, this RAG\nframework, denoted as M, can be defined as follows:\nM =\n\u0010\nG, R = (φ, ψ)\n\u0011\n, M(q; D) =G\n\u0010\nq, ψ(q; ˆD)\n\u0011\n, ˆD = φ(D) (1)\nIn this framework, G and R represent the generation module and the retrieval module, respectively,\nwhile q denotes the input query and D refers to the external database. The retrieval module R\n2\nQuery + LLM\nEntity Name:  Beekeeper\nEntity Type: PERSON\nDescription: A Beekeeper is \nan individual who produces ...\nOriginal Chunks ID: xxx\nSource: Honey Bee\nTarget: Industrial agriculture\nKeywords: Agriculture ...\nDescription:Honey Bees are \nnegatively impacted ...\nOriginal Chunks ID: xxx\n... BEEKEEPER‘spractices involve\nthe methods and strategies employed\nby beekeepers to manage bee\ncolonies and ensure their health and\nproductivity. A Beekeeper is an\nindividual who produces honey and\nother related products, playing a\ncrucial role in ……\nBeekeepers engage in various tasks,\nincluding observing bee behavior,\nmonitoring hive conditions,\npreventing pest infestations, and\nutilizing techniques to handle bees,\nsuch as using smoke to calm them ...\n Original Text\n-----Entities-----\n\"Beekeeper\",”A Beekeeper is an\nindividual who produces honey and\nother related products, playing a\ncrucial role in ......”\n-Relationships-\n\"Beekeeper\",“bee\",\"Beekeepers\nmanage bees but do not develop\nindividual relationships with them\ndue to the limited interaction time\nwith each hive.”\n----Contexts----\nBEEKEEPER's practices involve the\nmethods and strategies employed\nby beekeepers to manage ……\nGraph-based Text Indexing Dual-level Retrieval Paradigm\nEntity & Rel Extraction\nBeekeeper BeesObserve\nBeekeeper\nA beekeeper \nis an person \nwho…\nLLM Profiling\nBeekeeper beekeeper\nDeduplication\n…\nMatch\nIndex Graph \nused for Retrieval\nBeekeeper Honey Bee\n…Formers Hive\nLow-level Keys\nAgriculture Production\nEnvironmental Impact …\nHigh-level Keys\nEntities\nRelations\nRetrieved Content\nFigure 1: Overall architecture of the proposed LightRAG framework.\nincludes two key functionalities: i) Data Indexer φ(·): which involves building a specific data\nstructure ˆD based on the external database D. ii) Data Retriever ψ(·): The relevant documents are\nobtained by comparing the query against the indexed data, also denoted as “relevant documents”. By\nleveraging the information retrieved through ψ(·) along with the initial query q, the generative model\nG(·) efficiently produces high-quality, contextually relevant responses.\nIn this work, we target several key points essential for an efficient and effective Retrieval-Augmented\nGeneration (RAG) system which are elaborated below:\n• Comprehensive Information Retrieval: The indexing function φ(·) must be adept at extracting\nglobal information, as this is crucial for enhancing the model’s ability to answer queries effectively.\n• Efficient and Low-Cost Retrieval : The indexed data structure ˆD must enable rapid and cost-\nefficient retrieval to effectively handle a high volume of queries.\n• Fast Adaptation to Data Changes: The ability to swiftly and efficiently adjust the data structure\nto incorporate new information from the external knowledge base, is crucial for ensuring that the\nsystem remains current and relevant in an ever-changing information landscape.\n3 T HE LIGHT RAG A RCHITECTURE\n3.1 G RAPH -",
    "chunk_order_index": 1,
    "full_doc_id": "doc-45052d5e5c880b8ad46be8afde8266c9"
  },
  "chunk-5811df37bca65a0cb2913556357476bc": {
    "tokens": 1200,
    "content": "• Efficient and Low-Cost Retrieval : The indexed data structure ˆD must enable rapid and cost-\nefficient retrieval to effectively handle a high volume of queries.\n• Fast Adaptation to Data Changes: The ability to swiftly and efficiently adjust the data structure\nto incorporate new information from the external knowledge base, is crucial for ensuring that the\nsystem remains current and relevant in an ever-changing information landscape.\n3 T HE LIGHT RAG A RCHITECTURE\n3.1 G RAPH -BASED TEXT INDEXING\nGraph-Enhanced Entity and Relationship Extraction . Our LightRAG enhances the retrieval\nsystem by segmenting documents into smaller, more manageable pieces. This strategy allows for\nquick identification and access to relevant information without analyzing entire documents. Next,\nwe leverage LLMs to identify and extract various entities (e.g., names, dates, locations, and events)\nalong with the relationships between them. The information collected through this process will be\nused to create a comprehensive knowledge graph that highlights the connections and insights across\nthe entire collection of documents. We formally represent this graph generation module as follows:\nˆD = (ˆV, ˆE) =Dedupe ◦ Prof(V, E), V, E = ∪Di∈DRecog(Di) (2)\nwhere ˆD represents the resulting knowledge graphs. To generate this data, we apply three main\nprocessing steps to the raw text documents Di. These steps utilize a LLM for text analysis and\nprocessing. Details about the prompt templates and specific settings for this part can be found in\nAppendix 7.3.2. The functions used in our graph-based text indexing paradigm are described as:\n• Extracting Entities and Relationships. R(·): This function prompts a LLM to identify entities\n(nodes) and their relationships (edges) within the text data. For instance, it can extract entities\nlike \"Cardiologists\" and \"Heart Disease,\" and relationships such as \"Cardiologists diagnose Heart\nDisease\" from the text: \"Cardiologists assess symptoms to identify potential heart issues.\" To\nimprove efficiency, the raw textD is segmented into multiple chunks Di.\n• LLM Profiling for Key-Value Pair Generation. P(·): We employ a LLM-empowered profiling\nfunction, P(·), to generate a text key-value pair (K, V) for each entity node in V and relation\nedge in E. Each index key is a word or short phrase that enables efficient retrieval, while the\ncorresponding value is a text paragraph summarizing relevant snippets from external data to aid in\ntext generation. Entities use their names as the sole index key, whereas relations may have multiple\nindex keys derived from LLM enhancements that include global themes from connected entities.\n• Deduplication to Optimize Graph Operations . D(·): Finally, we implement a deduplication\nfunction, D(·), that identifies and merges identical entities and relations from different segments of\n3\nthe raw text Di. This process effectively reduces the overhead associated with graph operations on\nˆD by minimizing the graph’s size, leading to more efficient data processing.\nOur LightRAG offers two advantages through its graph-based text indexing paradigm. First, Com-\nprehensive Information Understanding. The constructed graph structures enable the extraction\nof global information from multi-hop subgraphs, greatly enhancing LightRAG’s ability to handle\ncomplex queries that span multiple document chunks. Second, Enhanced Retrieval Performance.\nthe key-value data structures derived from the graph are optimized for rapid and precise retrieval.\nThis provides a superior alternative to less accurate embedding matching methods (Gao et al., 2023)\nand inefficient chunk traversal techniques (Edge et al., 2024) commonly used in existing approaches.\nFast Adaptation to Incremental Knowledge Base. To efficiently adapt to evolving data changes\nwhile ensuring accurate and relevant responses, our LightRAG incrementally updates the knowledge\nbase without the need for complete reprocessing of the entire external database. For a new document\nD′, the incremental update algorithm processes it using the same graph-based indexing steps φ as\nbefore, resulting in ˆD′ = (ˆV′, ˆE′). Subsequently, LightRAGcombines the new graph data with the\noriginal by taking the union of the node sets ˆV and ˆV′, as well as the edge sets ˆE and ˆE′.\nTwo key objectives guide our approach to fast adaptation for the incremental knowledge base: Seam-\nless Integration of New Data . By applying a consistent methodology to new information, the\nincremental update module allows the LightRAG to integrate new external databases without disrupt-\ning the existing graph structure. This approach preserves the integrity of established connections,\nensuring that historical data remains accessible while enriching the graph without conflicts or redun-\ndancies. Reducing Computational Overhead . By eliminating the need to rebuild the entire index\ngraph, this method reduces computational overhead and facilitates the rapid assimilation of new data.\nConsequently, LightRAG maintains system accuracy, provides current information, and conserves\nresources, ensuring users receive timely updates and enhancing the overall RAG effectiveness.\n3.2 D UAL -LEVEL RETRIEVAL PARADIGM\nTo retrieve relevant information from both specific document chunks and their complex inter-\ndependencies, our LightRAG proposes generating query keys at both detailed and abstract levels.\n• Specific Queries. These queries are detail-oriented and typically reference specific entities within\nthe graph, requiring precise retrieval of information associated with particular nodes or edges. For\nexample, a specific query might be, “Who wrote ’Pride and Prejudice’?”\n• Abstract Queries. In contrast, abstract queries are more conceptual, encompassing broader topics,\nsummaries, or overarching themes that are not directly tied to specific entities. An example of an\nabstract query is, “",
    "chunk_order_index": 2,
    "full_doc_id": "doc-45052d5e5c880b8ad46be8afde8266c9"
  },
  "chunk-4b7ec4c901a260e0e9946185853e13c3": {
    "tokens": 1200,
    "content": "both detailed and abstract levels.\n• Specific Queries. These queries are detail-oriented and typically reference specific entities within\nthe graph, requiring precise retrieval of information associated with particular nodes or edges. For\nexample, a specific query might be, “Who wrote ’Pride and Prejudice’?”\n• Abstract Queries. In contrast, abstract queries are more conceptual, encompassing broader topics,\nsummaries, or overarching themes that are not directly tied to specific entities. An example of an\nabstract query is, “How does artificial intelligence influence modern education?”\nTo accommodate diverse query types, the LightRAG employs two distinct retrieval strategies within\nthe dual-level retrieval paradigm. This ensures that both specific and abstract inquiries are addressed\neffectively, allowing the system to deliver relevant responses tailored to user needs.\n• Low-Level Retrieval. This level is primarily focused on retrieving specific entities along with their\nassociated attributes or relationships. Queries at this level are detail-oriented and aim to extract\nprecise information about particular nodes or edges within the graph.\n• High-Level Retrieval. This level addresses broader topics and overarching themes. Queries at this\nlevel aggregate information across multiple related entities and relationships, providing insights\ninto higher-level concepts and summaries rather than specific details.\nIntegrating Graph and Vectors for Efficient Retrieval . By combining graph structures with\nvector representations, the model gains a deeper insight into the interrelationships among entities.\nThis synergy enables the retrieval algorithm to effectively utilize both local and global keywords,\nstreamlining the search process and improving the relevance of results.\n• (i) Query Keyword Extraction. For a given query q, the retrieval algorithm of LightRAG begins\nby extracting both local query keywords k(l) and global query keywords k(g).\n• (ii) Keyword Matching. The algorithm uses an efficient vector database to match local query\nkeywords with candidate entities and global query keywords with relations linked to global keys.\n4\n• (iii) Incorporating High-Order Relatedness. To enhance the query with higher-order relatedness,\nLightRAGfurther gathers neighboring nodes within the local subgraphs of the retrieved graph\nelements. This process involves the set {vi|vi ∈ V ∧(vi ∈ Nv ∨ vi ∈ Ne)}, where Nv and Ne\nrepresent the one-hop neighboring nodes of the retrieved nodes v and edges e, respectively.\nThis dual-level retrieval paradigm not only facilitates efficient retrieval of related entities and relations\nthrough keyword matching, but also enhances the comprehensiveness of results by integrating relevant\nstructural information from the constructed knowledge graph.\n3.3 R ETRIEVAL -AUGMENTED ANSWER GENERATION\nUtilization of Retrieved Information. Utilizing the retrieved information ψ(q; ˆD), our LightRAG\nemploys a general-purpose LLM to generate answers based on the collected data. This data comprises\nconcatenated values V from relevant entities and relations, produced by the profiling function P(·). It\nincludes names, descriptions of entities and relations, and excerpts from the original text.\nContext Integration and Answer Generation. By unifying the query with this multi-source text,\nthe LLM generates informative answers tailored to the user’s needs, ensuring alignment with the\nquery’s intent. This approach streamlines the answer generation process by integrating both context\nand query into the LLM model, as illustrated in detailed examples (Appendix 7.2).\n3.4 C OMPLEXITY ANALYSIS OF THE LIGHT RAG F RAMEWORK\nIn this section, we analyze the complexity of our proposed LightRAG framework, which can be\ndivided into two main parts. The first part is the graph-based Index phase. During this phase, we use\nthe large language model (LLM) to extract entities and relationships from each chunk of text. As\na result, the LLM needs to be called total tokens\nchunk size times. Importantly, there is no additional overhead\ninvolved in this process, making our approach highly efficient in managing updates to new text.\nThe second part of the process involves the graph-based retrieval phase. For each query, we first\nutilize the large language model (LLM) to generate relevant keywords. Similar to current Retrieval-\nAugmented Generation (RAG) systems Gao et al. (2023; 2022); Chan et al. (2024), our retrieval\nmechanism relies on vector-based search. However, instead of retrieving chunks as in conventional\nRAG, we concentrate on retrieving entities and relationships. This approach markedly reduces\nretrieval overhead compared to the community-based traversal method used in GraphRAG.\n4 E VALUATION\nWe conduct empirical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions:• (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? • (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? • (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? •\n(RQ4): What are the costs associated with LightRAG, as well as its adaptability to data changes?\n4.1 E XPERIMENTAL SETTINGS\nEvaluation Datasets. To conduct a comprehensive analysis of LightRAG, we selected four datasets\nfrom the UltraDomain benchmark (Qian et al., 2024). The UltraDomain data is sourced from 428\ncollege textbooks and encompasses 18 distinct domains, including agriculture, social sciences, and\nhumanities. From these, we chose the Agriculture, CS, Legal, and Mix datasets. Each dataset contains\nbetween 600,000 and 5,000,000 tokens, with detailed information provided in Table 4. Below is a\nspecific introduction to the four domains utilized in our experiments:\n• Agriculture: This domain focuses on agricultural practices",
    "chunk_order_index": 3,
    "full_doc_id": "doc-45052d5e5c880b8ad46be8afde8266c9"
  },
  "chunk-bc3d280001a458bb8f33320d18658757": {
    "tokens": 1200,
    "content": "2024). The UltraDomain data is sourced from 428\ncollege textbooks and encompasses 18 distinct domains, including agriculture, social sciences, and\nhumanities. From these, we chose the Agriculture, CS, Legal, and Mix datasets. Each dataset contains\nbetween 600,000 and 5,000,000 tokens, with detailed information provided in Table 4. Below is a\nspecific introduction to the four domains utilized in our experiments:\n• Agriculture: This domain focuses on agricultural practices, covering a range of topics including\nbeekeeping, hive management, crop production, and disease prevention.\n• CS: This domain focuses on computer science and encompasses key areas of data science and\nsoftware engineering. It particularly highlights machine learning and big data processing, featuring\ncontent on recommendation systems, classification algorithms, and real-time analytics using Spark.\n5\n• Legal: This domain centers on corporate legal practices, addressing corporate restructuring, legal\nagreements, regulatory compliance, and governance, with a focus on the legal and financial sectors.\n• Mixed: This domain presents a rich variety of literary, biographical, and philosophical texts,\nspanning a broad spectrum of disciplines, including cultural, historical, and philosophical studies.\nQuestion Generation. To evaluate the effectiveness of RAG systems for high-level sensemaking\ntasks, we consolidate all text content from each dataset as context and adopt the generation method\noutlined in Edge et al. (2024). Specifically, we instruct an LLM to generate five RAG users, along\nwith five tasks for each user. Each generated user is accompanied by a textual description detailing\ntheir expertise and traits that motivate their question-raising activities. Each user task is also described,\nemphasizing one of the user’s potential intentions when interacting with RAG systems. For each\nuser-task combination, the LLM generates five questions that require an understanding of the entire\ncorpus. In total, this process results in 125 questions for each dataset.\nBaselines. LightRAG is compared against the following state-of-the-art methods across all datasets:\n• Naive RAG (Gao et al., 2023): This model serves as a standard baseline in existing RAG systems.\nIt segments raw texts into chunks and stores them in a vector database using text embeddings. For\nqueries, Naive RAG generates vectorized representations to directly retrieve text chunks based on\nthe highest similarity in their representations, ensuring efficient and straightforward matching.\n• RQ-RAG (Chan et al., 2024): This approach leverages the LLM to decompose the input query\ninto multiple sub-queries. These sub-queries are designed to enhance search accuracy by utilizing\nexplicit techniques such as rewriting, decomposition, and disambiguation.\n• HyDE (Gao et al., 2022): This method utilizes the LLM to generate a hypothetical document based\non the input query. This generated document is then employed to retrieve relevant text chunks,\nwhich are subsequently used to formulate the final answer.\n• GraphRAG (Edge et al., 2024): This is a graph-enhanced RAG system that utilizes an LLM to\nextract entities and relationships from the text, representing them as nodes and edges. It generates\ncorresponding descriptions for these elements, aggregates nodes into communities, and produces a\ncommunity report to capture global information. When handling high-level queries, GraphRAG\nretrieves more comprehensive information by traversing these communities.\nImplementation and Evaluation Details. In our experiments, we utilize the nano vector database\nfor vector data management and access. For all LLM-based operations in LightRAG, we default\nto using GPT-4o-mini. To ensure consistency, the chunk size is set to 1200 across all datasets.\nAdditionally, the gleaning parameter is fixed at 1 for both GraphRAG and LightRAG.\nDefining ground truth for many RAG queries, particularly those involving complex high-level\nsemantics, poses significant challenges. To address this, we build on existing work (Edge et al.,\n2024) and adopt an LLM-based multi-dimensional comparison method. We employ a robust LLM,\nspecifically GPT-4o-mini, to rank each baseline against our LightRAG. The evaluation prompt we\nused is detailed in Appendix 7.3.4. In total, we utilize four evaluation dimensions, including:\ni) Comprehensiveness: How thoroughly does the answer address all aspects and details of the\nquestion? ii) Diversity: How varied and rich is the answer in offering different perspectives and\ninsights related to the question? iii) Empowerment: How effectively does the answer enable the\nreader to understand the topic and make informed judgments? iv) Overall: This dimension assesses\nthe cumulative performance across the three preceding criteria to identify the best overall answer.\nThe LLM directly compares two answers for each dimension and selects the superior response for\neach criterion. After identifying the winning answer for the three dimensions, the LLM combines the\nresults to determine the overall better answer. To ensure a fair evaluation and mitigate the potential\nbias that could arise from the order in which the answers are presented in the prompt, we alternate the\nplacement of each answer. We calculate win rates accordingly, ultimately leading to the final results.\n4.2 C OMPARISON OF LIGHT RAG WITH EXISTING RAG M ETHODS (RQ1)\nWe compare LightRAG against each baseline across various evaluation dimensions and datasets. The\nresults are presented in Table 1. Based on these findings, we draw the following conclusions:\n6\nTable 1: Win rates (%) of baselines v.s. LightRAG across four datasets and four evaluation dimensions.\nAgriculture CS Legal Mix\nNaiveRAGLightRAGNaiveRAGLightRAGNaiveRAGLightRAGNaiveRAGLightRAG\nComprehensiveness 32",
    "chunk_order_index": 4,
    "full_doc_id": "doc-45052d5e5c880b8ad46be8afde8266c9"
  },
  "chunk-9f8e3627478a2b09fd67b71faac55de0": {
    "tokens": 1200,
    "content": "RAG against each baseline across various evaluation dimensions and datasets. The\nresults are presented in Table 1. Based on these findings, we draw the following conclusions:\n6\nTable 1: Win rates (%) of baselines v.s. LightRAG across four datasets and four evaluation dimensions.\nAgriculture CS Legal Mix\nNaiveRAGLightRAGNaiveRAGLightRAGNaiveRAGLightRAGNaiveRAGLightRAG\nComprehensiveness 32.4% 67.6% 38.4% 61.6% 16.4% 83.6% 38.8% 61.2%\nDiversity 23.6% 76.4% 38.0% 62.0% 13.6% 86.4% 32.4% 67.6%\nEmpowerment 32.4% 67.6% 38.8% 61.2% 16.4% 83.6% 42.8% 57.2%\nOverall 32.4% 67.6% 38.8% 61.2% 15.2% 84.8% 40.0% 60.0%\nRQ-RAGLightRAGRQ-RAGLightRAGRQ-RAGLightRAGRQ-RAGLightRAG\nComprehensiveness 31.6% 68.4% 38.8% 61.2% 15.2% 84.8% 39.2% 60.8%\nDiversity 29.2% 70.8% 39.2% 60.8% 11.6% 88.4% 30.8% 69.2%\nEmpowerment 31.6% 68.4% 36.4% 63.6% 15.2% 84.8% 42.4% 57.6%\nOverall 32.4% 67.6% 38.0% 62.0% 14.4% 85.6% 40.0% 60.0%\nHyDE LightRAG HyDE LightRAG HyDE LightRAG HyDE LightRAG\nComprehensiveness 26.0% 74.0% 41.6% 58.4% 26.8% 73.2% 40.4% 59.6%\nDiversity 24.0% 76.0% 38.8% 61.2% 20.0% 80.0% 32.4% 67.6%\nEmpowerment 25.2% 74.8% 40.8% 59.2% 26.0% 74.0% 46.0% 54.0%\nOverall 24.8% 75.2% 41.6% 58.4% 26.4% 73.6% 42.4% 57.6%\nGraphRAGLightRAGGraphRAGLightRAGGraphRAGLightRAGGraphRAGLightRAG\nComprehensiveness 45.6% 54.4% 48.4% 51.6% 48.4% 51.6% 50.4% 49.6%\nDiversity 22.8% 77.2% 40.8% 59.2% 26.4% 73.6% 36.0% 64.0%\nEmpowerment 41.2% 58.8% 45.2% 54.8% 43.6% 56.4% 50.8% 49.2%\nOverall 45.2% 54.8% 48.0% 52.0% 47.2% 52.8% 50.4% 49.6%\nThe Superiority of Graph-enhanced RAG Systems in Large-Scale Corpora When handling large\ntoken counts and complex queries that require a thorough understanding of the dataset’s context,\ngraph-based RAG systems like LightRAG and GraphRAG consistently outperform purely chunk-\nbased retrieval methods such as NaiveRAG, HyDE, and RQRAG. This performance gap becomes\nparticularly pronounced as the dataset size increases. For instance, in the largest dataset (Legal), the\ndisparity widens significantly, with baseline methods achieving only about 20% win rates compared\nto the dominance of LightRAG. This trend underscores the advantages of graph-enhanced RAG\nsystems in capturing complex semantic dependencies within large-scale corpora, facilitating a more\ncomprehensive understanding of knowledge and leading to improved generalization performance.\nEnhancing Response Diversity with LightRAG: Compared to various baselines, LightRAG demon-\nstrates a significant advantage in the Diversity metric, particularly within the larger Legal dataset.\nIts consistent lead in this area underscores LightRAG’s effectiveness in generating a wider range\nof responses, especially in scenarios where diverse content is essential. We attribute this advantage\nto LightRAG’s dual-level retrieval paradigm, which facilitates comprehensive information retrieval\nfrom both low-level and high-level dimensions. This approach effectively leverages graph-based text\nindexing to consistently capture the full context in response to queries.\nLightRAG’s Superiority over GraphRAG: While both LightRAG and GraphRAG use graph-based\nretrieval mechanisms, LightRAG consistently outperforms GraphRAG, particularly in larger datasets\nwith complex language contexts. In the Agriculture, CS, and Legal datasets—each containing millions\nof tokens—LightRAG shows",
    "chunk_order_index": 5,
    "full_doc_id": "doc-45052d5e5c880b8ad46be8afde8266c9"
  },
  "chunk-738e35e30dadb213dc8631387b6e178b": {
    "tokens": 1200,
    "content": "both low-level and high-level dimensions. This approach effectively leverages graph-based text\nindexing to consistently capture the full context in response to queries.\nLightRAG’s Superiority over GraphRAG: While both LightRAG and GraphRAG use graph-based\nretrieval mechanisms, LightRAG consistently outperforms GraphRAG, particularly in larger datasets\nwith complex language contexts. In the Agriculture, CS, and Legal datasets—each containing millions\nof tokens—LightRAG shows a clear advantage, significantly surpassing GraphRAG and highlighting\nits strength in comprehensive information understanding within diverse environments. Enhanced\nResponse Variety: By integrating low-level retrieval of specific entities with high-level retrieval of\nbroader topics, LightRAG boosts response diversity. This dual-level mechanism effectively addresses\nboth detailed and abstract queries, ensuring a thorough grasp of information. Complex Query\nHandling: This approach is especially valuable in scenarios requiring diverse perspectives. By\naccessing both specific details and overarching themes, LightRAG adeptly responds to complex\nqueries involving interconnected topics, providing contextually relevant answers.\n4.3 A BLATION STUDIES (RQ2)\nWe also conduct ablation studies to evaluate the impact of our dual-level retrieval paradigm and the\neffectiveness of our graph-based text indexing in LightRAG. The results are presented in Table 2.\nEffectiveness of Dual-level Retrieval Paradigm. We begin by analyzing the effects of low-level and\nhigh-level retrieval paradigms. We compare two ablated models—each omitting one module—against\nLightRAG across four datasets. Here are our key observations for the different variants:\n7\nTable 2: Performance of ablated versions of LightRAG, using NaiveRAG as reference.\nAgriculture CS Legal Mix\nNaiveRAGLightRAGNaiveRAGLightRAGNaiveRAGLightRAGNaiveRAGLightRAG\nComprehensiveness 32.4% 67.6% 38.4% 61.6% 16.4% 83.6% 38.8% 61.2%Diversity 23.6% 76.4% 38.0% 62.0% 13.6% 86.4% 32.4% 67.6%Empowerment 32.4% 67.6% 38.8% 61.2% 16.4% 83.6% 42.8% 57.2%Overall 32.4% 67.6% 38.8% 61.2% 15.2% 84.8% 40.0% 60.0%\nNaiveRAG -High NaiveRAG -High NaiveRAG -High NaiveRAG -High\nComprehensiveness 34.8% 65.2% 42.8% 57.2% 23.6% 76.4% 40.4% 59.6%Diversity 27.2% 72.8% 36.8% 63.2% 16.8% 83.2% 36.0% 64.0%Empowerment 36.0% 64.0% 42.4% 57.6% 22.8% 77.2% 47.6% 52.4%Overall 35.2% 64.8% 44.0% 56.0% 22.0% 78.0% 42.4% 57.6%\nNaiveRAG -Low NaiveRAG -Low NaiveRAG -Low NaiveRAG -Low\nComprehensiveness 36.0% 64.0% 43.2% 56.8% 19.2% 80.8% 36.0% 64.0%Diversity 28.0% 72.0% 39.6% 60.4% 13.6% 86.4% 33.2% 66.8%Empowerment 34.8% 65.2% 42.8% 57.2% 16.4% 83.6% 35.2% 64.8%Overall 34.8% 65.2% 43.6% 56.4% 18.8% 81.2% 35.2% 64.8%\nNaiveRAG -Origin NaiveRAG -Origin NaiveRAG -Origin NaiveRAG -Origin\nComprehensiveness 24.8% 75.2% 39.2% 60.8% 16.4% 83.6% 44.4% 55.6%Diversity 26.4% 73.6% 44.8% 55.2% 14.4% 85.6% 25.6% 74.4%Empowerment 32.0% 68.0% 43.2% 56.8% 17.2% 82.8% 45.2% 54.8%Overall 25.6% 74.4% 39.2% 60.8% 15.6% 84.4% 44.4% 55.6%\n• Low-level-only Retrieval: The -High variant removes high-order retrieval, leading to a significant\nperformance decline across nearly all datasets and metrics. This drop is mainly due to its emphasis\non the specific information, which focuses excessively on entities and their immediate neighbors.\nWhile this approach enables deeper exploration",
    "chunk_order_index": 6,
    "full_doc_id": "doc-45052d5e5c880b8ad46be8afde8266c9"
  },
  "chunk-d01baacd18683c34cceed626b969003b": {
    "tokens": 1200,
    "content": "Overall 25.6% 74.4% 39.2% 60.8% 15.6% 84.4% 44.4% 55.6%\n• Low-level-only Retrieval: The -High variant removes high-order retrieval, leading to a significant\nperformance decline across nearly all datasets and metrics. This drop is mainly due to its emphasis\non the specific information, which focuses excessively on entities and their immediate neighbors.\nWhile this approach enables deeper exploration of directly related entities, it struggles to gather\ninformation for complex queries that demand comprehensive insights.\n• High-level-only Retrieval: The -Low variant prioritizes capturing a broader range of content by\nleveraging entity-wise relationships rather than focusing on specific entities. This approach offers\na significant advantage in comprehensiveness, allowing it to gather more extensive and varied\ninformation. However, the trade-off is a reduced depth in examining specific entities, which can\nlimit its ability to provide highly detailed insights. Consequently, this high-level-only retrieval\nmethod may struggle with tasks that require precise, detailed answers.\n• Hybrid Mode: The hybrid mode, or the full version of LightRAG, combines the strengths of\nboth low-level and high-level retrieval methods. It retrieves a broader set of relationships while\nsimultaneously conducting an in-depth exploration of specific entities. This dual-level approach\nensures both breadth in the retrieval process and depth in the analysis, providing a comprehensive\nview of the data. As a result, LightRAG achieves balanced performance across multiple dimensions.\nSemantic Graph Excels in RAG. We eliminated the use of original text in our retrieval process.\nSurprisingly, the resulting variant, -Origin, does not exhibit significant performance declines across\nall four datasets. In some cases, this variant even shows improvements (e.g. in Agriculture and Mix).\nWe attribute this phenomenon to the effective extraction of key information during the graph-based\nindexing process, which provides sufficient context for answering queries. Additionally, the original\ntext often contains irrelevant information that can introduce noise in the response.\n4.4 C ASE STUDY (RQ3)\nTo provide a clear comparison between baseline methods and our LightRAG, we present specific\ncase examples in Table 3, which includes responses to a machine learning question from both\nthe competitive baseline, GraphRAG, and our LightRAG framework. In this instance, LightRAG\noutperforms in all evaluation dimensions assessed by the LLM judge, including comprehensiveness,\ndiversity, empowerment, and overall quality. Our key observations are as follows:\ni) Comprehensiveness. Notably, LightRAG covers a broader range of machine learning metrics,\nshowcasing its comprehensiveness and ability to effectively discover relevant information. This\nhighlights the strength of our graph-based indexing paradigm, which excels in precise entity and\nrelation extraction as well as LLM profiling. ii) Both Diversity and Empowerment. Furthermore,\nLightRAG not only offers a more diverse array of information but also delivers more empowering\ncontent. This success is due to LightRAG’s hierarchical retrieval paradigm, which combines in-depth\n8\nTable 3: Case Study: Comparison Between LightRAG and the Baseline Method GraphRAG.\nQuery Which methods can normalize the feature values for effective machine learning?\nGraphRAG Key Metrics for Evaluating Movie Recommendation Systems: Evaluating the effectiveness of movie\nrecommendation systems is crucial for ensuring user satisfaction and improving the overall performance of\nthese systems. Several metrics are widely recognized for their ability to provide insights into how well a\nrecommendation system performs. Below are the most informative metrics: 1.Precision[...] 2.Recall[...]\n3.F1 Score[...] 4.Mean Average Precision (MAP)[...] 5.Root Mean Squared Error (RMSE)[...] 6.\nUser Satisfaction Metrics[...]Conclusion: In summary, metrics such as precision, recall, F1 score, MAP,\nRMSE, and user satisfaction are critical for evaluating the performance of movie recommendation systems.\nEach metric provides unique insights that can help developers and researchers refine their algorithms and\nenhance user experiences. By focusing on these metrics, stakeholders can ensure that their recommendation\nsystems effectively meet user preferences and expectations.\nLightRAG Key Metrics for Evaluating Movie Recommendation Systems: Evaluating movie recommendation systems\ninvolves multiple metrics that assess the effectiveness and accuracy of the recommendations provided to\nusers. Below are some of the most informative metrics commonly used in this context, derived from the\ndata and concepts surrounding recommendation systems. 1.Mean Average Precision at K (MAPK)[...] 2.\nPrecision and Recall[...] 3.Root Mean Squared Error (RMSE) and Mean Squared Error (MSE)[...]\n4.Area Under the Curve (AUC)[...] 5.F-Measure[...] 6.User Engagement Metrics[...]Conclusion: A\ncomprehensive evaluation of movie recommendation systems employs a range of metrics that capture both\nthe accuracy of the recommendations and user satisfaction. By effectively leveraging these metrics, system\ndesigners can optimize recommendation engines to deliver more personalized and relevant experiences.\nLLM DecisionComprehensiveness: \"Winner\": \"Answer 2 (LightRAG)\",\n\"Explanation\": \"Answer 2 provides a broader array of metrics including MAPK, AUC, and user engagement\nmetrics alongside precision, recall, RMSE, and F-Measure. This gives a fuller view of how recommendation\nsystems can be evaluated. Answer 1, while detailed, is more focused on traditional metrics and lacks the\ninclusion of engagement metrics which are crucial for understanding user interaction.\"\nDiversity: \"Winner\": \"Answer 2 (LightRAG)\",\n\"Explanation\": \"Answer 2 not only covers a wide variety of metrics but also includes nuanced explanations\nof how some metrics interrelate and differ from one another, like the inclusion of both RM",
    "chunk_order_index": 7,
    "full_doc_id": "doc-45052d5e5c880b8ad46be8afde8266c9"
  },
  "chunk-9444bf125e11d7eefd3bcbe1a9fc7327": {
    "tokens": 1200,
    "content": ". This gives a fuller view of how recommendation\nsystems can be evaluated. Answer 1, while detailed, is more focused on traditional metrics and lacks the\ninclusion of engagement metrics which are crucial for understanding user interaction.\"\nDiversity: \"Winner\": \"Answer 2 (LightRAG)\",\n\"Explanation\": \"Answer 2 not only covers a wide variety of metrics but also includes nuanced explanations\nof how some metrics interrelate and differ from one another, like the inclusion of both RMSE and MSE, as\nwell as the details behind AUC. In contrast, Answer 1 sticks primarily to standard metrics without much\nexploration of potential nuances.\"\nEmpowerment: \"Winner\": \"Answer 2 (LightRAG)\",\n\"Explanation\": \"Answer 2 empowers the reader more effectively by detailing how each metric functions and\nits importance in evaluating recommendation systems. By providing context such as the trade-offs between\nprecision and recall and emphasizing user engagement metrics, it enables readers to make more informed\njudgments and understand the implications of different metrics. Answer 1 is more straightforward but lacks\nthe depth of insight regarding why these metrics matter.\"\nOverall Winner: \"Winner\": \"Answer 2 (LightRAG)\",\n\"Explanation\": \"While Answer 1 is more direct and systematic, Answer 2 excels in comprehensiveness,\ndiversity, and empowerment. It provides a richer exploration of the topic, including insights into user\nengagement and nuanced differences between metrics. This depth and breadth make it more informative for\nreaders seeking to thoroughly understand the evaluation of movie recommendation systems.\"\nexplorations of related entities through low-level retrieval to enhance empowerment with broader\nexplorations via high-level retrieval to improve answer diversity. Together, these approaches capture a\ncomprehensive global perspective of the knowledge domain, contributing to better RAG performance.\n4.5 M ODEL COST AND ADAPTABILITY ANALYSIS (RQ4)\nFigure 2: Comparison of Cost in Terms of To-\nkens and API Calls for GraphRAG and Ligh-\ntRAG on the Legal Dataset.\nPhase Retrieval Phase Incremental Text Update\nModel GraphRAG Ours GraphRAG Ours\nTokens 610 × 1,000 < 100 1,399 × 2 × 5,000 Textract+Textract\nAPI 610×1,000\nCmax 1 1,399 × 2 +Cextract CextractCalls\nWe compare the cost of our LightRAG with that of the\ntop-performing baseline, GraphRAG, from two key\nperspectives. First, we examine the number of tokens\nand API calls during the indexing and retrieval pro-\ncesses. Second, we analyze these metrics in relation\nto handling data changes in dynamic environments.\nThe results of this evaluation on the legal dataset are\npresented in Table 2. In this context, Textract repre-\nsents the token overhead for entity and relationship extraction, Cmax denotes the maximum number\nof tokens allowed per API call, and Cextract indicates the number of API calls required for extraction.\nIn the retrieval phase, GraphRAG generates 1,399 communities, with 610 level-2 communities\nactively utilized for retrieval in this experiment. Each community report averages 1,000 tokens,\nresulting in a total token consumption of 610,000 tokens (610 communities × 1,000 tokens per\ncommunity). Additionally, GraphRAG’s requirement to traverse each community individually leads\nto hundreds of API calls, significantly increasing retrieval overhead. In contrast, LightRAG optimizes\nthis process by using fewer than 100 tokens for keyword generation and retrieval, requiring only a\nsingle API call for the entire process. This efficiency is achieved through our retrieval mechanism,\n9\nwhich seamlessly integrates graph structures and vectorized representations for information retrieval,\nthereby eliminating the need to process large volumes of information upfront.\nIn the incremental data update phase, designed to address changes in dynamic real-world scenarios,\nboth models exhibit similar overhead for entity and relationship extraction. However, GraphRAG\nshows significant inefficiency in managing newly added data. When a new dataset of the same\nsize as the legal dataset is introduced, GraphRAG must dismantle its existing community structure\nto incorporate new entities and relationships, followed by complete regeneration. This process\nincurs a substantial token cost of approximately 5,000 tokens per community report. Given 1,399\ncommunities, GraphRAG would require around 1,399 × 2 × 5,000 tokens to reconstruct both the\noriginal and new community reports—an exorbitant expense that underscores its inefficiency. In\ncontrast, LightRAG seamlessly integrates newly extracted entities and relationships into the existing\ngraph without the need for full reconstruction. This approach results in significantly lower overhead\nduring incremental updates, demonstrating its superior efficiency and cost-effectiveness.\n5 R ELATED WORK\n5.1 R ETRIEVAL -AUGMENTED GENERATION WITH LLM S\nRetrieval-Augmented Generation (RAG) systems enhance LLM inputs by retrieving relevant infor-\nmation from external sources, grounding responses in factual, domain-specific knowledge Ram et al.\n(2023); Fan et al. (2024). Current RAG approaches Gao et al. (2022; 2023); Chan et al. (2024); Yu\net al. (2024) typically embed queries in a vector space to find the nearest context vectors. However,\nmany of these methods rely on fragmented text chunks and only retrieve the top-k contexts, limiting\ntheir ability to capture comprehensive global information needed for effective responses.\nAlthough recent studies Edge et al. (2024) have explored using graph structures for knowledge\nrepresentation, two key limitations persist. First, these approaches often lack the capability for\ndynamic updates and expansions of the knowledge graph, making it difficult to",
    "chunk_order_index": 8,
    "full_doc_id": "doc-45052d5e5c880b8ad46be8afde8266c9"
  },
  "chunk-86bccbbe0cccdf77d049219c4c7e5e6a": {
    "tokens": 1200,
    "content": "(2024) typically embed queries in a vector space to find the nearest context vectors. However,\nmany of these methods rely on fragmented text chunks and only retrieve the top-k contexts, limiting\ntheir ability to capture comprehensive global information needed for effective responses.\nAlthough recent studies Edge et al. (2024) have explored using graph structures for knowledge\nrepresentation, two key limitations persist. First, these approaches often lack the capability for\ndynamic updates and expansions of the knowledge graph, making it difficult to incorporate new\ninformation effectively. In contrast, our proposed model, LightRAG, addresses these challenges\nby enabling the RAG system to quickly adapt to new information, ensuring the model’s timeliness\nand accuracy. Additionally, existing methods often rely on brute-force searches for each generated\ncommunity, which are inefficient for large-scale queries. Our LightRAG framework overcomes this\nlimitation by facilitating rapid retrieval of relevant information from the graph through our proposed\ndual-level retrieval paradigm, significantly enhancing both retrieval efficiency and response speed.\n5.2 L ARGE LANGUAGE MODEL FOR GRAPHS\nGraphs are a powerful framework for representing complex relationships and find applications\nin numerous fields. As Large Language Models (LLMs) continue to evolve, researchers have\nincreasingly focused on enhancing their capability to interpret graph-structured data. This body of\nwork can be divided into three primary categories: i) GNNs as Prefix where Graph Neural Networks\n(GNNs) are utilized as the initial processing layer for graph data, generating structure-aware tokens\nthat LLMs can use during inference. Notable examples include GraphGPT Tang et al. (2024) and\nLLaGA Chen et al. (2024). ii) LLMs as Prefix involves LLMs processing graph data enriched with\ntextual information to produce node embeddings or labels, ultimately refining the training process\nfor GNNs, as demonstrated in systems like GALM Xie et al. (2023) and OFA Liu et al. (2024). iii)\nLLMs-Graphs Integration focuses on achieving a seamless interaction between LLMs and graph\ndata, employing techniques such as fusion training and GNN alignment, and developing LLM-based\nagents capable of engaging with graph information directly Li et al. (2023); Brannon et al. (2023).\n6 C ONCLUSION\nThis work introduces an advancement in Retrieval-Augmented Generation (RAG) through the\nintegration of a graph-based indexing approach that enhances both efficiency and comprehension\nin information retrieval. LightRAG utilizes a comprehensive knowledge graph to facilitate rapid\nand relevant document retrieval, enabling a deeper understanding of complex queries. Its dual-level\nretrieval paradigm allows for the extraction of both specific and abstract information, catering to\ndiverse user needs. Furthermore, LightRAG’s seamless incremental update capability ensures that\nthe system remains current and responsive to new information, thereby maintaining its effectiveness\nover time. Overall, LightRAG excels in both efficiency and effectiveness, significantly improving the\nspeed and quality of information retrieval and generation while reducing costs for LLM inference.\n10\nREFERENCES\nWilliam Brannon, Suyash Fulay, Hang Jiang, Wonjune Kang, Brandon Roy, Jad Kabbara, and Deb\nRoy. Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings. arXiv\npreprint arXiv:2305.14321, 2023.\nChi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. Rq-rag:\nLearning to refine queries for retrieval augmented generation. arXiv preprint arXiv:2404.00610,\n2024.\nRunjin Chen, Tong Zhao, AJAY KUMAR JAISW AL, Neil Shah, and Zhangyang Wang. Llaga: Large\nlanguage and graph assistant. In International Conference on Machine Learning (ICML), 2024.\nDarren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt,\nand Jonathan Larson. From local to global: A graph rag approach to query-focused summarization.\narXiv preprint arXiv:2404.16130, 2024.\nShahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. Ragas: Automated evaluation\nof retrieval augmented generation. In International Conference of the European Chapter of the\nAssociation for Computational Linguistics (EACL), pp. 150–158, 2024.\nWenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and\nQing Li. A survey on rag meeting llms: Towards retrieval-augmented large language models. In\nInternational Conference on Knowledge Discovery and Data Mining (KDD), pp. 6491–6501, 2024.\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without\nrelevance labels. arXiv preprint arXiv:2212.10496, 2022.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and\nHaofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv\npreprint arXiv:2312.10997, 2023.\nYichuan Li, Kaize Ding, and Kyumin Lee. Grenade: Graph-centric language model for self-\nsupervised representation learning on text-attributed graphs. In International Conference on\nEmpirical Methods in Natural Language Processing (EMNLP",
    "chunk_order_index": 9,
    "full_doc_id": "doc-45052d5e5c880b8ad46be8afde8266c9"
  },
  "chunk-c9b168cc278b7236096162736b592f49": {
    "tokens": 1200,
    "content": "uxi Bi, Yi Dai, Jiawei Sun, and\nHaofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv\npreprint arXiv:2312.10997, 2023.\nYichuan Li, Kaize Ding, and Kyumin Lee. Grenade: Graph-centric language model for self-\nsupervised representation learning on text-attributed graphs. In International Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), pp. 2745–2757, 2023.\nHao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, and Muhan Zhang.\nOne for all: Towards training one graph model for all classification tasks. In International\nConference on Learning Representations (ICLR), 2024.\nYuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong\nLiu, Tong Xu, and Enhong Chen. Crud-rag: A comprehensive chinese benchmark for retrieval-\naugmented generation of large language models. arXiv preprint arXiv:2401.17043, 2024.\nHongjin Qian, Peitian Zhang, Zheng Liu, Kelong Mao, and Zhicheng Dou. Memorag: Moving\ntowards next-gen rag via memory-inspired knowledge discovery, 2024. URL https://arxiv.\norg/abs/2409.05591.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and\nYoav Shoham. In-context retrieval-augmented language models. Transactions of the Association\nfor Computational Linguistics (TACL), 11:1316–1331, 2023.\nLadislav Rampášek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Do-\nminique Beaini. Recipe for a general, powerful, scalable graph transformer. International\nConference on Neural Information Processing Systems (NeurIPS), 35:14501–14515, 2022.\nAlireza Salemi and Hamed Zamani. Evaluating retrieval quality in retrieval-augmented generation.\nIn ACM International Conference on Research and Development in Information Retrieval (SIGIR),\npp. 2395–2400, 2024.\nViju Sudhi, Sinchana Ramakanth Bhat, Max Rudat, and Roman Teucher. Rag-ex: A generic\nframework for explaining retrieval augmented generation. In ACM International Conference on\nResearch and Development in Information Retrieval (SIGIR), pp. 2776–2780, 2024.\n11\nJiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang.\nGraphgpt: Graph instruction tuning for large language models. In ACM International Conference\non Research and Development in Information Retrieval (SIGIR), pp. 491–500, 2024.\nShangqing Tu, Yuanchun Wang, Jifan Yu, Yuyang Xie, Yaran Shi, Xiaozhi Wang, Jing Zhang, Lei\nHou, and Juanzi Li. R-eval: A unified toolkit for evaluating domain knowledge of retrieval\naugmented large language models. In International Conference on Knowledge Discovery and Data\nMining (KDD), pp. 5813–5824, 2024.\nHan Xie, Da Zheng, Jun Ma, Houyu Zhang, Vassilis N Ioannidis, Xiang Song, Qing Ping, Sheng\nWang, Carl Yang, Yi Xu, et al. Graph-aware language model pre-training on a large graph corpus\ncan help multiple graph applications. In International Conference on Knowledge Discovery and\nData Mining (KDD), pp. 5270–5281, 2023.\nYue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and\nBryan Catanzaro. Rankrag: Unifying context ranking with retrieval-augmented generation in llms.\narXiv preprint arXiv:2407.02485, 2024.\nPenghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang,\nWentao Zhang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A survey.\narXiv preprint arXiv:2402.19473, 2024.\n12\n7 A PPENDIX\nIn this section, we elaborate on the methodologies and experimental settings used in the LightRAG\nframework. It describes the specific steps for extracting entities and relationships from documents,\ndetailing how large language models (LLMs) are utilized for this purpose. The section also specifies\nthe prompt templates and configurations used in LLM operations, ensuring clarity in the experimental\nsetup. Additionally, it outlines the evaluation criteria and dimensions used to assess the performance\nof LightRAG against baselines from various dimensions.\n7.1 E XPERIMENTAL DATA DETAILS\nTable 4: Statistical information of the datasets.\nStatistics Agriculture CS Legal Mix\nTotal Documents 12 10 94 61\nTotal Tokens 2,017,886 2,306,535 5,081,069 619,009\nTable 4 presents statistical information for four datasets: Agriculture, CS, Legal, and Mix. The\nAgriculture dataset consists of 12 documents totaling 2,017,886 tokens, while the CS dataset contains\n10 documents with 2,306,535 tokens. The Legal dataset is the largest",
    "chunk_order_index": 10,
    "full_doc_id": "doc-45052d5e5c880b8ad46be8afde8266c9"
  },
  "chunk-a0819113d1bd5861ea3ba0a48542183f": {
    "tokens": 1200,
    "content": "Legal Mix\nTotal Documents 12 10 94 61\nTotal Tokens 2,017,886 2,306,535 5,081,069 619,009\nTable 4 presents statistical information for four datasets: Agriculture, CS, Legal, and Mix. The\nAgriculture dataset consists of 12 documents totaling 2,017,886 tokens, while the CS dataset contains\n10 documents with 2,306,535 tokens. The Legal dataset is the largest, comprising 94 documents and\n5,081,069 tokens. Lastly, the Mix dataset includes 61 documents with a total of 619,009 tokens.\n7.2 C ASE EXAMPLE OF RETRIEVAL -AUGMENTED GENERATION IN LIGHT RAG.\nFigure 3: A retrieval and generation example.\nIn Figure 3, we illustrate the retrieve-and-generate process. When presented with the query, “What\nmetrics are most informative for evaluating movie recommendation systems?”, the LLM first extracts\nboth low-level and high-level keywords. These keywords guide the dual-level retrieval process on the\n13\ngenerated knowledge graph, targeting relevant entities and relationships. The retrieved information\nis organized into three components: entities, relationships, and corresponding text chunks. This\nstructured data is then fed into the LLM, enabling it to generate a comprehensive answer to the query.\n7.3 O VERVIEW OF THE PROMPTS USED IN LIGHT RAG\n7.3.1 P ROMPTS FOR GRAPH GENERATION\nFigure 4: Prompts for Graph Generation\nThe graph construction prompt outlined in Figure 4 is designed to extract and structure entity-\nrelationship information from a text document based on specified entity types. The process begins\nby identifying entities and categorizing them into types such as organization, person, location, and\nevent. It then provides detailed descriptions of their attributes and activities. Next, the prompt\nidentifies relationships between these entities, offering explanations, assigning strength scores, and\nsummarizing the relationships using high-level keywords.\n7.3.2 P ROMPTS FOR QUERY GENERATION\nFigure 5: Prompts for Query Generation\nIn Figure 5, the query generation prompt outlines a framework for identifying potential user roles\n(e.g., data scientist, finance analyst, and product manager) and their objectives for generating queries\nbased on a specified dataset description. The prompt explains how to define five distinct users\nwho would benefit from interacting with the dataset. For each user, it specifies five key tasks they\nwould perform while working with the dataset. Additionally, for each (user, task) combination, five\nhigh-level questions are posed to ensure a thorough understanding of the dataset.\n14\n7.3.3 P ROMPTS FOR KEYWORD EXTRACTION\nFigure 6: Prompts for Keyword Extraction\nIn Figure 6, the prompt describes a method for extracting keywords from a user’s query, distinguishing\nbetween high-level and low-level keywords. High-level keywords represent broad concepts or\nthemes, while low-level keywords focus on specific entities and details. The extracted keywords are\nreturned in JSON format, organized into two fields: “high_level_keywords” for overarching ideas\nand “low_level_keywords” for specific details.\n7.3.4 P ROMPTS FOR RAG E VALUATION\nFigure 7: Prompts for RAG Evaluation\nThe evaluation prompt is illustrated in Figure 7. It introduces a comprehensive evaluation framework\nfor comparing two answers to the same question based on three key criteria: Comprehensiveness,\nDiversity, and Empowerment. Its purpose is to guide the LLM through the process of selecting the\nbetter answer for each criterion, followed by an overall assessment. For each of the three criteria, the\n15\nTable 5: Case Study: Comparison Between LightRAG and the Baseline NaiveRAG.\nQuery How do indigenous perspectives on ownership and collaboration influence corporate mergers in countries\nlike Canada and Australia?\nNaiveRAG Indigenous perspectives on ownership and collaboration play a significant role in shaping corporate mergers\nin countries like Canada and Australia, particularly where resources and land rights intersect with Indige-\nnous sovereignty and cultural values. These perspectives emphasize community engagement, respect for\ntraditional land use, and a collaborative approach to resource management, influencing corporate strategies\nand outcomes.\n. . .\nBoth nations illustrate that prioritizing Indigenous perspectives leads to more sustainable and equitable\noutcomes. Acknowledging Indigenous rights and fostering collaborative relationships can reduce conflicts\nand enhance corporate reputations. Ultimately, integrating Indigenous perspectives is a pathway to better\nbusiness practices and community relations.\nLightRAG Indigenous Perspectives on Ownership and Collaboration in Corporate Mergers\nIndigenous perspectives significantly influence corporate mergers in countries like Canada and Australia.\nThese views emphasize communal rights to land and resources, spiritual connections to the environment, and\nthe importance of collaboration over competition.\nCultural Significance of Land Ownership[...]\nThe Role of Collaboration[...]\nLegal and Regulatory Frameworks[...]\nConclusion\nIndigenous perspectives shape corporate mergers by emphasizing communal ownership and collaborative\npartnerships. Corporations must adopt respectful, inclusive practices that honor Indigenous cultural values\nand rights, paving the way for sustainable, mutually beneficial outcomes.\nLLM DecisionComprehensiveness: \"Winner\": \"Answer 2 (LightRAG)\",\n\"Explanation\": \"Answer 2 provides a thorough exploration of the indigenous perspectives by discussing\ncultural significance, collaboration, and legal frameworks, including specific examples and detailed insights.\nIn contrast, Answer 1, while informative, lacks the same depth in analyzing the various dimensions of the\nindigenous perspectives on ownership and collaboration.\"\nDiversity: \"Winner\": \"Answer 2 (LightRAG)\",\n\"Explanation\": \"Answer 2 presents a wider array of perspectives, focusing on the communal aspect of\nland ownership, spiritual connections, and practical examples of collaboration. It covers more ground by\ncontr",
    "chunk_order_index": 11,
    "full_doc_id": "doc-45052d5e5c880b8ad46be8afde8266c9"
  },
  "chunk-0c7c163214ba79d9ee58bf5111bfa033": {
    "tokens": 552,
    "content": "significance, collaboration, and legal frameworks, including specific examples and detailed insights.\nIn contrast, Answer 1, while informative, lacks the same depth in analyzing the various dimensions of the\nindigenous perspectives on ownership and collaboration.\"\nDiversity: \"Winner\": \"Answer 2 (LightRAG)\",\n\"Explanation\": \"Answer 2 presents a wider array of perspectives, focusing on the communal aspect of\nland ownership, spiritual connections, and practical examples of collaboration. It covers more ground by\ncontrasting Indigenous views with Western notions, whereas Answer 1 primarily focuses on corporate\nstrategies and outcomes with limited perspective.\"\nEmpowerment: \"Winner\": \"Answer 2 (LightRAG)\",\n\"Explanation\": \"Answer 2 equips the reader with nuanced understanding and actionable insights by\nhighlighting the importance of collaboration and the legal frameworks involved. It empowers corporations\nby illustrating how adopting an inclusive approach can facilitate better outcomes. Answer 1, while practical,\ndoes not emphasize the moral or ethical implications as strongly.\"\nOverall Winner: \"Winner\": \"Answer 2 (LightRAG)\",\n\"Explanation\": \"Answer 2 excels overall due to its comprehensive exploration, diversity of perspectives,\nand empowerment of the reader with actionable insights about indigenous perspectives and collaboration\nin corporate mergers. Although Answer 1 is more direct, the depth and breadth of Answer 2 make it the\nstronger response.\"\nLLM must identify which answer performs better and provide a rationale for its choice. Ultimately,\nan overall winner is determined based on performance across all three dimensions, accompanied by a\ndetailed summary that justifies the decision. The evaluation is structured in JSON format, ensuring\nclarity and consistency, and facilitating a systematic comparison between the two answers.\n7.4 C ASE STUDY: C OMPARISON BETWEEN LIGHT RAG AND THE BASELINE NAIVE RAG.\nTo further illustrate LightRAG’s superiority over baseline models in terms of comprehensiveness,\nempowerment, and diversity, we present a case study comparing LightRAG and NaiveRAG in\nTable 5. This study addresses a question regarding indigenous perspectives in the context of corporate\nmergers. Notably, LightRAG offers a more in-depth exploration of key themes related to indigenous\nperspectives, such as cultural significance, collaboration, and legal frameworks, supported by specific\nand illustrative examples. In contrast, while NaiveRAG provides informative responses, it lacks\nthe depth needed to thoroughly examine the various dimensions of indigenous ownership and\ncollaboration. The dual-level retrieval process employed by LightRAG enables a more comprehensive\ninvestigation of specific entities and their interrelationships, facilitating extensive searches that\neffectively capture overarching themes and complexities within the topic.\n16",
    "chunk_order_index": 12,
    "full_doc_id": "doc-45052d5e5c880b8ad46be8afde8266c9"
  }
}