{
  "doc-b69071ce61333d60732925e3da60adb0": {
    "content": "# source: lightrag/kg/neo4j_impl.py\n# content_type: functions_classes\n# language: python\n\nclass Neo4JStorage(BaseGraphStorage):\n    @staticmethod\n    def load_nx_graph(file_name):\n        print(\"no preloading of graph with neo4j in production\")\n\n    def __init__(self, namespace, global_config):\n        super().__init__(namespace=namespace, global_config=global_config)\n        self._driver = None\n        self._driver_lock = asyncio.Lock()\n        URI = os.environ[\"NEO4J_URI\"]\n        USERNAME = os.environ[\"NEO4J_USERNAME\"]\n        PASSWORD = os.environ[\"NEO4J_PASSWORD\"]\n        self._driver: AsyncDriver = AsyncGraphDatabase.driver(\n            URI, auth=(USERNAME, PASSWORD)\n        )\n        return None\n\n    def __post_init__(self):\n        self._node_embed_algorithms = {\n            \"node2vec\": self._node2vec_embed,\n        }\n\n    async def close(self):\n        if self._driver:\n            await self._driver.close()\n            self._driver = None\n\n    async def __aexit__(self, exc_type, exc, tb):\n        if self._driver:\n            await self._driver.close()\n\n    async def index_done_callback(self):\n        print(\"KG successfully indexed.\")\n\n    async def has_node(self, node_id: str) -> bool:\n        entity_name_label = node_id.strip('\"')\n\n        async with self._driver.session() as session:\n            query = (\n                f\"MATCH (n:`{entity_name_label}`) RETURN count(n) > 0 AS node_exists\"\n            )\n            result = await session.run(query)\n            single_result = await result.single()\n            logger.debug(\n                f'{inspect.currentframe().f_code.co_name}:query:{query}:result:{single_result[\"node_exists\"]}'\n            )\n            return single_result[\"node_exists\"]\n\n    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:\n        entity_name_label_source = source_node_id.strip('\"')\n        entity_name_label_target = target_node_id.strip('\"')\n\n        async with self._driver.session() as session:\n            query = (\n                f\"MATCH (a:`{entity_name_label_source}`)-[r]-(b:`{entity_name_label_target}`) \"\n                \"RETURN COUNT(r) > 0 AS edgeExists\"\n            )\n            result = await session.run(query)\n            single_result = await result.single()\n            logger.debug(\n                f'{inspect.currentframe().f_code.co_name}:query:{query}:result:{single_result[\"edgeExists\"]}'\n            )\n            return single_result[\"edgeExists\"]\n\n    async def get_node(self, node_id: str) -> Union[dict, None]:\n        async with self._driver.session() as session:\n            entity_name_label = node_id.strip('\"')\n            query = f\"MATCH (n:`{entity_name_label}`) RETURN n\"\n            result = await session.run(query)\n            record = await result.single()\n            if record:\n                node = record[\"n\"]\n                node_dict = dict(node)\n                logger.debug(\n                    f\"{inspect.currentframe().f_code.co_name}: query: {query}, result: {node_dict}\"\n                )\n                return node_dict\n            return None\n\n    async def node_degree(self, node_id: str) -> int:\n        entity_name_label = node_id.strip('\"')\n\n        async with self._driver.session() as session:\n            query = f\"\"\"\n                MATCH (n:`{entity_name_label}`)\n                RETURN COUNT{{ (n)--() }} AS totalEdgeCount\n            \"\"\"\n            result = await session.run(query)\n            record = await result.single()\n            if record:\n                edge_count = record[\"totalEdgeCount\"]\n                logger.debug(\n                    f\"{inspect.currentframe().f_code.co_name}:query:{query}:result:{edge_count}\"\n                )\n                return edge_count\n            else:\n                return None\n\n    async def edge_degree(self, src_id: str, tgt_id: str) -> int:\n        entity_name_label_source = src_id.strip('\"')\n        entity_name_label_target = tgt_id.strip('\"')\n        src_degree = await self.node_degree(entity_name_label_source)\n        trg_degree = await self.node_degree(entity_name_label_target)\n\n        # Convert None to 0 for addition\n        src_degree = 0 if src_degree is None else src_degree\n        trg_degree = 0 if trg_degree is None else trg_degree\n\n        degrees = int(src_degree) + int(trg_degree)\n        logger.debug(\n            f\"{inspect.currentframe().f_code.co_name}:query:src_Degree+trg_degree:result:{degrees}\"\n        )\n        return degrees\n\n    async def get_edge(\n        self, source_node_id: str, target_node_id: str\n    ) -> Union[dict, None]:\n        entity_name_label_source = source_node_id.strip('\"')\n        entity_name_label_target = target_node_id.strip('\"')\n        \"\"\"\n        Find all edges between nodes of two given labels\n\n        Args:\n            source_node_label (str): Label of the source nodes\n            target_node_label (str): Label of the target nodes\n\n        Returns:\n            list: List of all relationships/edges found\n        \"\"\"\n        async with self._driver.session() as session:\n            query = f\"\"\"\n            MATCH (start:`{entity_name_label_source}`)-[r]->(end:`{entity_name_label_target}`)\n            RETURN properties(r) as edge_properties\n            LIMIT 1\n            \"\"\".format(\n                entity_name_label_source=entity_name_label_source,\n                entity_name_label_target=entity_name_label_target,\n            )\n\n            result = await session.run(query)\n            record = await result.single()\n            if record:\n                result = dict(record[\"edge_properties\"])\n                logger.debug(\n                    f\"{inspect.currentframe().f_code.co_name}:query:{query}:result:{result}\"\n                )\n                return result\n            else:\n                return None\n\n    async def get_node_edges(self, source_node_id: str) -> List[Tuple[str, str]]:\n        node_label = source_node_id.strip('\"')\n\n        \"\"\"\n        Retrieves all edges (relationships) for a particular node identified by its label.\n        :return: List of dictionaries containing edge information\n        \"\"\"\n        query = f\"\"\"MATCH (n:`{node_label}`)\n                OPTIONAL MATCH (n)-[r]-(connected)\n                RETURN n, r, connected\"\"\"\n        async with self._driver.session() as session:\n            results = await session.run(query)\n            edges = []\n            async for record in results:\n                source_node = record[\"n\"]\n                connected_node = record[\"connected\"]\n\n                source_label = (\n                    list(source_node.labels)[0] if source_node.labels else None\n                )\n                target_label = (\n                    list(connected_node.labels)[0]\n                    if connected_node and connected_node.labels\n                    else None\n                )\n\n                if source_label and target_label:\n                    edges.append((source_label, target_label))\n\n            return edges\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type(\n            (\n                neo4jExceptions.ServiceUnavailable,\n                neo4jExceptions.TransientError,\n                neo4jExceptions.WriteServiceUnavailable,\n                neo4jExceptions.ClientError,\n            )\n        ),\n    )\n    async def upsert_node(self, node_id: str, node_data: Dict[str, Any]):\n        \"\"\"\n        Upsert a node in the Neo4j database.\n\n        Args:\n            node_id: The unique identifier for the node (used as label)\n            node_data: Dictionary of node properties\n        \"\"\"\n        label = node_id.strip('\"')\n        properties = node_data\n\n        async def _do_upsert(tx: AsyncManagedTransaction):\n            query = f\"\"\"\n            MERGE (n:`{label}`)\n            SET n += $properties\n            \"\"\"\n            await tx.run(query, properties=properties)\n            logger.debug(\n                f\"Upserted node with label '{label}' and properties: {properties}\"\n            )\n\n        try:\n            async with self._driver.session() as session:\n                await session.execute_write(_do_upsert)\n        except Exception as e:\n            logger.error(f\"Error during upsert: {str(e)}\")\n            raise\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type(\n            (\n                neo4jExceptions.ServiceUnavailable,\n                neo4jExceptions.TransientError,\n                neo4jExceptions.WriteServiceUnavailable,\n            )\n        ),\n    )\n    async def upsert_edge(\n        self, source_node_id: str, target_node_id: str, edge_data: Dict[str, Any]\n    ):\n        \"\"\"\n        Upsert an edge and its properties between two nodes identified by their labels.\n\n        Args:\n            source_node_id (str): Label of the source node (used as identifier)\n            target_node_id (str): Label of the target node (used as identifier)\n            edge_data (dict): Dictionary of properties to set on the edge\n        \"\"\"\n        source_node_label = source_node_id.strip('\"')\n        target_node_label = target_node_id.strip('\"')\n        edge_properties = edge_data\n\n        async def _do_upsert_edge(tx: AsyncManagedTransaction):\n            query = f\"\"\"\n            MATCH (source:`{source_node_label}`)\n            WITH source\n            MATCH (target:`{target_node_label}`)\n            MERGE (source)-[r:DIRECTED]->(target)\n            SET r += $properties\n            RETURN r\n            \"\"\"\n            await tx.run(query, properties=edge_properties)\n            logger.debug(\n                f\"Upserted edge from '{source_node_label}' to '{target_node_label}' with properties: {edge_properties}\"\n            )\n\n        try:\n            async with self._driver.session() as session:\n                await session.execute_write(_do_upsert_edge)\n        except Exception as e:\n            logger.error(f\"Error during edge upsert: {str(e)}\")\n            raise\n\n    async def _node2vec_embed(self):\n        print(\"Implemented but never called.\")"
  },
  "doc-dd805a7d3f0c30f94be96a499b9288bb": {
    "content": "# source: lightrag/kg/neo4j_impl.py\n# content_type: simplified_code\n# language: python\n\nimport asyncio\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any, Union, Tuple, List, Dict\nimport inspect\nfrom lightrag.utils import logger\nfrom ..base import BaseGraphStorage\nfrom neo4j import (\n    AsyncGraphDatabase,\n    exceptions as neo4jExceptions,\n    AsyncDriver,\n    AsyncManagedTransaction,\n)\n\n\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_exponential,\n    retry_if_exception_type,\n)\n\n\n@dataclass\n# Code for: class Neo4JStorage(BaseGraphStorage):"
  },
  "doc-f0e6d273aafbc657de6a368f7fc6c1ea": {
    "content": "# source: lightrag/kg/oracle_impl.py\n# content_type: functions_classes\n# language: python\n\nclass OracleDB:\n    def __init__(self, config, **kwargs):\n        self.host = config.get(\"host\", None)\n        self.port = config.get(\"port\", None)\n        self.user = config.get(\"user\", None)\n        self.password = config.get(\"password\", None)\n        self.dsn = config.get(\"dsn\", None)\n        self.config_dir = config.get(\"config_dir\", None)\n        self.wallet_location = config.get(\"wallet_location\", None)\n        self.wallet_password = config.get(\"wallet_password\", None)\n        self.workspace = config.get(\"workspace\", None)\n        self.max = 12\n        self.increment = 1\n        logger.info(f\"Using the label {self.workspace} for Oracle Graph as identifier\")\n        if self.user is None or self.password is None:\n            raise ValueError(\"Missing database user or password in addon_params\")\n\n        try:\n            oracledb.defaults.fetch_lobs = False\n\n            self.pool = oracledb.create_pool_async(\n                user=self.user,\n                password=self.password,\n                dsn=self.dsn,\n                config_dir=self.config_dir,\n                wallet_location=self.wallet_location,\n                wallet_password=self.wallet_password,\n                min=1,\n                max=self.max,\n                increment=self.increment,\n            )\n            logger.info(f\"Connected to Oracle database at {self.dsn}\")\n        except Exception as e:\n            logger.error(f\"Failed to connect to Oracle database at {self.dsn}\")\n            logger.error(f\"Oracle database error: {e}\")\n            raise\n\n    def numpy_converter_in(self, value):\n        \"\"\"Convert numpy array to array.array\"\"\"\n        if value.dtype == np.float64:\n            dtype = \"d\"\n        elif value.dtype == np.float32:\n            dtype = \"f\"\n        else:\n            dtype = \"b\"\n        return array.array(dtype, value)\n\n    def input_type_handler(self, cursor, value, arraysize):\n        \"\"\"Set the type handler for the input data\"\"\"\n        if isinstance(value, np.ndarray):\n            return cursor.var(\n                oracledb.DB_TYPE_VECTOR,\n                arraysize=arraysize,\n                inconverter=self.numpy_converter_in,\n            )\n\n    def numpy_converter_out(self, value):\n        \"\"\"Convert array.array to numpy array\"\"\"\n        if value.typecode == \"b\":\n            dtype = np.int8\n        elif value.typecode == \"f\":\n            dtype = np.float32\n        else:\n            dtype = np.float64\n        return np.array(value, copy=False, dtype=dtype)\n\n    def output_type_handler(self, cursor, metadata):\n        \"\"\"Set the type handler for the output data\"\"\"\n        if metadata.type_code is oracledb.DB_TYPE_VECTOR:\n            return cursor.var(\n                metadata.type_code,\n                arraysize=cursor.arraysize,\n                outconverter=self.numpy_converter_out,\n            )\n\n    async def check_tables(self):\n        for k, v in TABLES.items():\n            try:\n                if k.lower() == \"lightrag_graph\":\n                    await self.query(\n                        \"SELECT id FROM GRAPH_TABLE (lightrag_graph MATCH (a) COLUMNS (a.id)) fetch first row only\"\n                    )\n                else:\n                    await self.query(\"SELECT 1 FROM {k}\".format(k=k))\n            except Exception as e:\n                logger.error(f\"Failed to check table {k} in Oracle database\")\n                logger.error(f\"Oracle database error: {e}\")\n                try:\n                    # print(v[\"ddl\"])\n                    await self.execute(v[\"ddl\"])\n                    logger.info(f\"Created table {k} in Oracle database\")\n                except Exception as e:\n                    logger.error(f\"Failed to create table {k} in Oracle database\")\n                    logger.error(f\"Oracle database error: {e}\")\n\n        logger.info(\"Finished check all tables in Oracle database\")\n\n    async def query(\n        self, sql: str, params: dict = None, multirows: bool = False\n    ) -> Union[dict, None]:\n        async with self.pool.acquire() as connection:\n            connection.inputtypehandler = self.input_type_handler\n            connection.outputtypehandler = self.output_type_handler\n            with connection.cursor() as cursor:\n                try:\n                    await cursor.execute(sql, params)\n                except Exception as e:\n                    logger.error(f\"Oracle database error: {e}\")\n                    print(sql)\n                    print(params)\n                    raise\n                columns = [column[0].lower() for column in cursor.description]\n                if multirows:\n                    rows = await cursor.fetchall()\n                    if rows:\n                        data = [dict(zip(columns, row)) for row in rows]\n                    else:\n                        data = []\n                else:\n                    row = await cursor.fetchone()\n                    if row:\n                        data = dict(zip(columns, row))\n                    else:\n                        data = None\n                return data\n\n    async def execute(self, sql: str, data: list | dict = None):\n        # logger.info(\"go into OracleDB execute method\")\n        try:\n            async with self.pool.acquire() as connection:\n                connection.inputtypehandler = self.input_type_handler\n                connection.outputtypehandler = self.output_type_handler\n                with connection.cursor() as cursor:\n                    if data is None:\n                        await cursor.execute(sql)\n                    else:\n                        # print(data)\n                        # print(sql)\n                        await cursor.execute(sql, data)\n                    await connection.commit()\n        except Exception as e:\n            logger.error(f\"Oracle database error: {e}\")\n            print(sql)\n            print(data)\n            raise"
  },
  "doc-047287d5e4aa5695c1efb8f9d7050b88": {
    "content": "# source: lightrag/kg/oracle_impl.py\n# content_type: functions_classes\n# language: python\n\nclass OracleKVStorage(BaseKVStorage):\n    # should pass db object to self.db\n    def __post_init__(self):\n        self._data = {}\n        self._max_batch_size = self.global_config[\"embedding_batch_num\"]\n\n    ################ QUERY METHODS ################\n\n    async def get_by_id(self, id: str) -> Union[dict, None]:\n        \"\"\"根据 id 获取 doc_full 数据.\"\"\"\n        SQL = SQL_TEMPLATES[\"get_by_id_\" + self.namespace]\n        params = {\"workspace\": self.db.workspace, \"id\": id}\n        # print(\"get_by_id:\"+SQL)\n        res = await self.db.query(SQL, params)\n        if res:\n            data = res  # {\"data\":res}\n            # print (data)\n            return data\n        else:\n            return None\n\n    # Query by id\n    async def get_by_ids(self, ids: list[str], fields=None) -> Union[list[dict], None]:\n        \"\"\"根据 id 获取 doc_chunks 数据\"\"\"\n        SQL = SQL_TEMPLATES[\"get_by_ids_\" + self.namespace].format(\n            ids=\",\".join([f\"'{id}'\" for id in ids])\n        )\n        params = {\"workspace\": self.db.workspace}\n        # print(\"get_by_ids:\"+SQL)\n        # print(params)\n        res = await self.db.query(SQL, params, multirows=True)\n        if res:\n            data = res  # [{\"data\":i} for i in res]\n            # print(data)\n            return data\n        else:\n            return None\n\n    async def filter_keys(self, keys: list[str]) -> set[str]:\n        \"\"\"过滤掉重复内容\"\"\"\n        SQL = SQL_TEMPLATES[\"filter_keys\"].format(\n            table_name=N_T[self.namespace], ids=\",\".join([f\"'{id}'\" for id in keys])\n        )\n        params = {\"workspace\": self.db.workspace}\n        try:\n            await self.db.query(SQL, params)\n        except Exception as e:\n            logger.error(f\"Oracle database error: {e}\")\n            print(SQL)\n            print(params)\n        res = await self.db.query(SQL, params, multirows=True)\n        data = None\n        if res:\n            exist_keys = [key[\"id\"] for key in res]\n            data = set([s for s in keys if s not in exist_keys])\n        else:\n            exist_keys = []\n            data = set([s for s in keys if s not in exist_keys])\n        return data\n\n    ################ INSERT METHODS ################\n    async def upsert(self, data: dict[str, dict]):\n        left_data = {k: v for k, v in data.items() if k not in self._data}\n        self._data.update(left_data)\n        # print(self._data)\n        # values = []\n        if self.namespace == \"text_chunks\":\n            list_data = [\n                {\n                    \"__id__\": k,\n                    **{k1: v1 for k1, v1 in v.items()},\n                }\n                for k, v in data.items()\n            ]\n            contents = [v[\"content\"] for v in data.values()]\n            batches = [\n                contents[i : i + self._max_batch_size]\n                for i in range(0, len(contents), self._max_batch_size)\n            ]\n            embeddings_list = await asyncio.gather(\n                *[self.embedding_func(batch) for batch in batches]\n            )\n            embeddings = np.concatenate(embeddings_list)\n            for i, d in enumerate(list_data):\n                d[\"__vector__\"] = embeddings[i]\n            # print(list_data)\n            for item in list_data:\n                merge_sql = SQL_TEMPLATES[\"merge_chunk\"]\n                data = {\n                    \"check_id\": item[\"__id__\"],\n                    \"id\": item[\"__id__\"],\n                    \"content\": item[\"content\"],\n                    \"workspace\": self.db.workspace,\n                    \"tokens\": item[\"tokens\"],\n                    \"chunk_order_index\": item[\"chunk_order_index\"],\n                    \"full_doc_id\": item[\"full_doc_id\"],\n                    \"content_vector\": item[\"__vector__\"],\n                }\n                # print(merge_sql)\n                await self.db.execute(merge_sql, data)\n\n        if self.namespace == \"full_docs\":\n            for k, v in self._data.items():\n                # values.clear()\n                merge_sql = SQL_TEMPLATES[\"merge_doc_full\"]\n                data = {\n                    \"check_id\": k,\n                    \"id\": k,\n                    \"content\": v[\"content\"],\n                    \"workspace\": self.db.workspace,\n                }\n                # print(merge_sql)\n                await self.db.execute(merge_sql, data)\n        return left_data\n\n    async def index_done_callback(self):\n        if self.namespace in [\"full_docs\", \"text_chunks\"]:\n            logger.info(\"full doc and chunk data had been saved into oracle db!\")"
  },
  "doc-73fd087c22538f216324c7f3360826e0": {
    "content": "# source: lightrag/kg/oracle_impl.py\n# content_type: functions_classes\n# language: python\n\nclass OracleVectorDBStorage(BaseVectorStorage):\n    cosine_better_than_threshold: float = 0.2\n\n    def __post_init__(self):\n        pass\n\n    async def upsert(self, data: dict[str, dict]):\n        \"\"\"向向量数据库中插入数据\"\"\"\n        pass\n\n    async def index_done_callback(self):\n        pass\n\n    #################### query method ###############\n    async def query(self, query: str, top_k=5) -> Union[dict, list[dict]]:\n        \"\"\"从向量数据库中查询数据\"\"\"\n        embeddings = await self.embedding_func([query])\n        embedding = embeddings[0]\n        # 转换精度\n        dtype = str(embedding.dtype).upper()\n        dimension = embedding.shape[0]\n        embedding_string = \"[\" + \", \".join(map(str, embedding.tolist())) + \"]\"\n\n        SQL = SQL_TEMPLATES[self.namespace].format(dimension=dimension, dtype=dtype)\n        params = {\n            \"embedding_string\": embedding_string,\n            \"workspace\": self.db.workspace,\n            \"top_k\": top_k,\n            \"better_than_threshold\": self.cosine_better_than_threshold,\n        }\n        # print(SQL)\n        results = await self.db.query(SQL, params=params, multirows=True)\n        # print(\"vector search result:\",results)\n        return results"
  },
  "doc-c4c2ee97b3d2ce28b61f23733f31e62b": {
    "content": "# source: lightrag/kg/oracle_impl.py\n# content_type: functions_classes\n# language: python\n\nclass OracleGraphStorage(BaseGraphStorage):\n    \"\"\"基于Oracle的图存储模块\"\"\"\n\n    def __post_init__(self):\n        \"\"\"从graphml文件加载图\"\"\"\n        self._max_batch_size = self.global_config[\"embedding_batch_num\"]\n\n    #################### insert method ################\n\n    async def upsert_node(self, node_id: str, node_data: dict[str, str]):\n        \"\"\"插入或更新节点\"\"\"\n        # print(\"go into upsert node method\")\n        entity_name = node_id\n        entity_type = node_data[\"entity_type\"]\n        description = node_data[\"description\"]\n        source_id = node_data[\"source_id\"]\n        logger.debug(f\"entity_name:{entity_name}, entity_type:{entity_type}\")\n\n        content = entity_name + description\n        contents = [content]\n        batches = [\n            contents[i : i + self._max_batch_size]\n            for i in range(0, len(contents), self._max_batch_size)\n        ]\n        embeddings_list = await asyncio.gather(\n            *[self.embedding_func(batch) for batch in batches]\n        )\n        embeddings = np.concatenate(embeddings_list)\n        content_vector = embeddings[0]\n        merge_sql = SQL_TEMPLATES[\"merge_node\"]\n        data = {\n            \"workspace\": self.db.workspace,\n            \"name\": entity_name,\n            \"entity_type\": entity_type,\n            \"description\": description,\n            \"source_chunk_id\": source_id,\n            \"content\": content,\n            \"content_vector\": content_vector,\n        }\n        # print(merge_sql)\n        await self.db.execute(merge_sql, data)\n        # self._graph.add_node(node_id, **node_data)\n\n    async def upsert_edge(\n        self, source_node_id: str, target_node_id: str, edge_data: dict[str, str]\n    ):\n        \"\"\"插入或更新边\"\"\"\n        # print(\"go into upsert edge method\")\n        source_name = source_node_id\n        target_name = target_node_id\n        weight = edge_data[\"weight\"]\n        keywords = edge_data[\"keywords\"]\n        description = edge_data[\"description\"]\n        source_chunk_id = edge_data[\"source_id\"]\n        logger.debug(\n            f\"source_name:{source_name}, target_name:{target_name}, keywords: {keywords}\"\n        )\n\n        content = keywords + source_name + target_name + description\n        contents = [content]\n        batches = [\n            contents[i : i + self._max_batch_size]\n            for i in range(0, len(contents), self._max_batch_size)\n        ]\n        embeddings_list = await asyncio.gather(\n            *[self.embedding_func(batch) for batch in batches]\n        )\n        embeddings = np.concatenate(embeddings_list)\n        content_vector = embeddings[0]\n        merge_sql = SQL_TEMPLATES[\"merge_edge\"]\n        data = {\n            \"workspace\": self.db.workspace,\n            \"source_name\": source_name,\n            \"target_name\": target_name,\n            \"weight\": weight,\n            \"keywords\": keywords,\n            \"description\": description,\n            \"source_chunk_id\": source_chunk_id,\n            \"content\": content,\n            \"content_vector\": content_vector,\n        }\n        # print(merge_sql)\n        await self.db.execute(merge_sql, data)\n        # self._graph.add_edge(source_node_id, target_node_id, **edge_data)\n\n    async def embed_nodes(self, algorithm: str) -> tuple[np.ndarray, list[str]]:\n        \"\"\"为节点生成向量\"\"\"\n        if algorithm not in self._node_embed_algorithms:\n            raise ValueError(f\"Node embedding algorithm {algorithm} not supported\")\n        return await self._node_embed_algorithms[algorithm]()\n\n    async def _node2vec_embed(self):\n        \"\"\"为节点生成向量\"\"\"\n        from graspologic import embed\n\n        embeddings, nodes = embed.node2vec_embed(\n            self._graph,\n            **self.config[\"node2vec_params\"],\n        )\n\n        nodes_ids = [self._graph.nodes[node_id][\"id\"] for node_id in nodes]\n        return embeddings, nodes_ids\n\n    async def index_done_callback(self):\n        \"\"\"写入graphhml图文件\"\"\"\n        logger.info(\n            \"Node and edge data had been saved into oracle db already, so nothing to do here!\"\n        )\n\n    #################### query method #################\n    async def has_node(self, node_id: str) -> bool:\n        \"\"\"根据节点id检查节点是否存在\"\"\"\n        SQL = SQL_TEMPLATES[\"has_node\"]\n        params = {\"workspace\": self.db.workspace, \"node_id\": node_id}\n        # print(SQL)\n        # print(self.db.workspace, node_id)\n        res = await self.db.query(SQL, params)\n        if res:\n            # print(\"Node exist!\",res)\n            return True\n        else:\n            # print(\"Node not exist!\")\n            return False\n\n    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:\n        \"\"\"根据源和目标节点id检查边是否存在\"\"\"\n        SQL = SQL_TEMPLATES[\"has_edge\"]\n        params = {\n            \"workspace\": self.db.workspace,\n            \"source_node_id\": source_node_id,\n            \"target_node_id\": target_node_id,\n        }\n        # print(SQL)\n        res = await self.db.query(SQL, params)\n        if res:\n            # print(\"Edge exist!\",res)\n            return True\n        else:\n            # print(\"Edge not exist!\")\n            return False\n\n    async def node_degree(self, node_id: str) -> int:\n        \"\"\"根据节点id获取节点的度\"\"\"\n        SQL = SQL_TEMPLATES[\"node_degree\"]\n        params = {\"workspace\": self.db.workspace, \"node_id\": node_id}\n        # print(SQL)\n        res = await self.db.query(SQL, params)\n        if res:\n            # print(\"Node degree\",res[\"degree\"])\n            return res[\"degree\"]\n        else:\n            # print(\"Edge not exist!\")\n            return 0\n\n    async def edge_degree(self, src_id: str, tgt_id: str) -> int:\n        \"\"\"根据源和目标节点id获取边的度\"\"\"\n        degree = await self.node_degree(src_id) + await self.node_degree(tgt_id)\n        # print(\"Edge degree\",degree)\n        return degree\n\n    async def get_node(self, node_id: str) -> Union[dict, None]:\n        \"\"\"根据节点id获取节点数据\"\"\"\n        SQL = SQL_TEMPLATES[\"get_node\"]\n        params = {\"workspace\": self.db.workspace, \"node_id\": node_id}\n        # print(self.db.workspace, node_id)\n        # print(SQL)\n        res = await self.db.query(SQL, params)\n        if res:\n            # print(\"Get node!\",self.db.workspace, node_id,res)\n            return res\n        else:\n            # print(\"Can't get node!\",self.db.workspace, node_id)\n            return None\n\n    async def get_edge(\n        self, source_node_id: str, target_node_id: str\n    ) -> Union[dict, None]:\n        \"\"\"根据源和目标节点id获取边\"\"\"\n        SQL = SQL_TEMPLATES[\"get_edge\"]\n        params = {\n            \"workspace\": self.db.workspace,\n            \"source_node_id\": source_node_id,\n            \"target_node_id\": target_node_id,\n        }\n        res = await self.db.query(SQL, params)\n        if res:\n            # print(\"Get edge!\",self.db.workspace, source_node_id, target_node_id,res[0])\n            return res\n        else:\n            # print(\"Edge not exist!\",self.db.workspace, source_node_id, target_node_id)\n            return None\n\n    async def get_node_edges(self, source_node_id: str):\n        \"\"\"根据节点id获取节点的所有边\"\"\"\n        if await self.has_node(source_node_id):\n            SQL = SQL_TEMPLATES[\"get_node_edges\"]\n            params = {\"workspace\": self.db.workspace, \"source_node_id\": source_node_id}\n            res = await self.db.query(sql=SQL, params=params, multirows=True)\n            if res:\n                data = [(i[\"source_name\"], i[\"target_name\"]) for i in res]\n                # print(\"Get node edge!\",self.db.workspace, source_node_id,data)\n                return data\n            else:\n                # print(\"Node Edge not exist!\",self.db.workspace, source_node_id)\n                return []\n\n    async def get_all_nodes(self, limit: int):\n        \"\"\"查询所有节点\"\"\"\n        SQL = SQL_TEMPLATES[\"get_all_nodes\"]\n        params = {\"workspace\": self.db.workspace, \"limit\": str(limit)}\n        res = await self.db.query(sql=SQL, params=params, multirows=True)\n        if res:\n            return res\n\n    async def get_all_edges(self, limit: int):\n        \"\"\"查询所有边\"\"\"\n        SQL = SQL_TEMPLATES[\"get_all_edges\"]\n        params = {\"workspace\": self.db.workspace, \"limit\": str(limit)}\n        res = await self.db.query(sql=SQL, params=params, multirows=True)\n        if res:\n            return res\n\n    async def get_statistics(self):\n        SQL = SQL_TEMPLATES[\"get_statistics\"]\n        params = {\"workspace\": self.db.workspace}\n        res = await self.db.query(sql=SQL, params=params, multirows=True)\n        if res:\n            return res"
  },
  "doc-4261a241010e5834e653c49515667408": {
    "content": "# source: lightrag/kg/oracle_impl.py\n# content_type: simplified_code\n# language: python\n\nimport asyncio\n\n# import html\n# import os\nfrom dataclasses import dataclass\nfrom typing import Union\nimport numpy as np\nimport array\n\nfrom ..utils import logger\nfrom ..base import (\n    BaseGraphStorage,\n    BaseKVStorage,\n    BaseVectorStorage,\n)\n\nimport oracledb\n\n\n# Code for: class OracleDB:\n\n\n@dataclass\n# Code for: class OracleKVStorage(BaseKVStorage):\n\n\n@dataclass\n# Code for: class OracleVectorDBStorage(BaseVectorStorage):\n\n\n@dataclass\n# Code for: class OracleGraphStorage(BaseGraphStorage):\n\n\nN_T = {\n    \"full_docs\": \"LIGHTRAG_DOC_FULL\",\n    \"text_chunks\": \"LIGHTRAG_DOC_CHUNKS\",\n    \"chunks\": \"LIGHTRAG_DOC_CHUNKS\",\n    \"entities\": \"LIGHTRAG_GRAPH_NODES\",\n    \"relationships\": \"LIGHTRAG_GRAPH_EDGES\",\n}\n\nTABLES = {\n    \"LIGHTRAG_DOC_FULL\": {\n        \"ddl\": \"\"\"CREATE TABLE LIGHTRAG_DOC_FULL (\n                    id varchar(256)PRIMARY KEY,\n                    workspace varchar(1024),\n                    doc_name varchar(1024),\n                    content CLOB,\n                    meta JSON,\n                    createtime TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    updatetime TIMESTAMP DEFAULT NULL\n                    )\"\"\"\n    },\n    \"LIGHTRAG_DOC_CHUNKS\": {\n        \"ddl\": \"\"\"CREATE TABLE LIGHTRAG_DOC_CHUNKS (\n                    id varchar(256) PRIMARY KEY,\n                    workspace varchar(1024),\n                    full_doc_id varchar(256),\n                    chunk_order_index NUMBER,\n                    tokens NUMBER,\n                    content CLOB,\n                    content_vector VECTOR,\n                    createtime TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    updatetime TIMESTAMP DEFAULT NULL\n                    )\"\"\"\n    },\n    \"LIGHTRAG_GRAPH_NODES\": {\n        \"ddl\": \"\"\"CREATE TABLE LIGHTRAG_GRAPH_NODES (\n                    id NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n                    workspace varchar(1024),\n                    name varchar(2048),\n                    entity_type varchar(1024),\n                    description CLOB,\n                    source_chunk_id varchar(256),\n                    content CLOB,\n                    content_vector VECTOR,\n                    createtime TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    updatetime TIMESTAMP DEFAULT NULL\n                    )\"\"\"\n    },\n    \"LIGHTRAG_GRAPH_EDGES\": {\n        \"ddl\": \"\"\"CREATE TABLE LIGHTRAG_GRAPH_EDGES (\n                    id NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n                    workspace varchar(1024),\n                    source_name varchar(2048),\n                    target_name varchar(2048),\n                    weight NUMBER,\n                    keywords CLOB,\n                    description CLOB,\n                    source_chunk_id varchar(256),\n                    content CLOB,\n                    content_vector VECTOR,\n                    createtime TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    updatetime TIMESTAMP DEFAULT NULL\n                    )\"\"\"\n    },\n    \"LIGHTRAG_LLM_CACHE\": {\n        \"ddl\": \"\"\"CREATE TABLE LIGHTRAG_LLM_CACHE (\n                    id varchar(256) PRIMARY KEY,\n                    send clob,\n                    return clob,\n                    model varchar(1024),\n                    createtime TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    updatetime TIMESTAMP DEFAULT NULL\n                    )\"\"\"\n    },\n    \"LIGHTRAG_GRAPH\": {\n        \"ddl\": \"\"\"CREATE OR REPLACE PROPERTY GRAPH lightrag_graph\n                VERTEX TABLES (\n                    lightrag_graph_nodes KEY (id)\n                        LABEL entity\n                        PROPERTIES (id,workspace,name) -- ,entity_type,description,source_chunk_id)\n                )\n                EDGE TABLES (\n                    lightrag_graph_edges KEY (id)\n                        SOURCE KEY (source_name) REFERENCES lightrag_graph_nodes(name)\n                        DESTINATION KEY (target_name) REFERENCES lightrag_graph_nodes(name)\n                        LABEL  has_relation\n                        PROPERTIES (id,workspace,source_name,target_name) -- ,weight, keywords,description,source_chunk_id)\n                ) OPTIONS(ALLOW MIXED PROPERTY TYPES)\"\"\"\n    },\n}\n\n\nSQL_TEMPLATES = {\n    # SQL for KVStorage\n    \"get_by_id_full_docs\": \"select ID,NVL(content,'') as content from LIGHTRAG_DOC_FULL where workspace=:workspace and ID=:id\",\n    \"get_by_id_text_chunks\": \"select ID,TOKENS,NVL(content,'') as content,CHUNK_ORDER_INDEX,FULL_DOC_ID from LIGHTRAG_DOC_CHUNKS where workspace=:workspace and ID=:id\",\n    \"get_by_ids_full_docs\": \"select ID,NVL(content,'') as content from LIGHTRAG_DOC_FULL where workspace=:workspace and ID in ({ids})\",\n    \"get_by_ids_text_chunks\": \"select ID,TOKENS,NVL(content,'') as content,CHUNK_ORDER_INDEX,FULL_DOC_ID  from LIGHTRAG_DOC_CHUNKS where workspace=:workspace and ID in ({ids})\",\n    \"filter_keys\": \"select id from {table_name} where workspace=:workspace and id in ({ids})\",\n    \"merge_doc_full\": \"\"\" MERGE INTO LIGHTRAG_DOC_FULL a\n                    USING DUAL\n                    ON (a.id = :check_id)\n                    WHEN NOT MATCHED THEN\n                    INSERT(id,content,workspace) values(:id,:content,:workspace)\n                    \"\"\",\n    \"merge_chunk\": \"\"\"MERGE INTO LIGHTRAG_DOC_CHUNKS a\n                    USING DUAL\n                    ON (a.id = :check_id)\n                    WHEN NOT MATCHED THEN\n                    INSERT(id,content,workspace,tokens,chunk_order_index,full_doc_id,content_vector)\n                    values (:id,:content,:workspace,:tokens,:chunk_order_index,:full_doc_id,:content_vector) \"\"\",\n    # SQL for VectorStorage\n    \"entities\": \"\"\"SELECT name as entity_name FROM\n        (SELECT id,name,VECTOR_DISTANCE(content_vector,vector(:embedding_string,{dimension},{dtype}),COSINE) as distance\n        FROM LIGHTRAG_GRAPH_NODES WHERE workspace=:workspace)\n        WHERE distance>:better_than_threshold ORDER BY distance ASC FETCH FIRST :top_k ROWS ONLY\"\"\",\n    \"relationships\": \"\"\"SELECT source_name as src_id, target_name as tgt_id FROM\n        (SELECT id,source_name,target_name,VECTOR_DISTANCE(content_vector,vector(:embedding_string,{dimension},{dtype}),COSINE) as distance\n        FROM LIGHTRAG_GRAPH_EDGES WHERE workspace=:workspace)\n        WHERE distance>:better_than_threshold ORDER BY distance ASC FETCH FIRST :top_k ROWS ONLY\"\"\",\n    \"chunks\": \"\"\"SELECT id FROM\n        (SELECT id,VECTOR_DISTANCE(content_vector,vector(:embedding_string,{dimension},{dtype}),COSINE) as distance\n        FROM LIGHTRAG_DOC_CHUNKS WHERE workspace=:workspace)\n        WHERE distance>:better_than_threshold ORDER BY distance ASC FETCH FIRST :top_k ROWS ONLY\"\"\",\n    # SQL for GraphStorage\n    \"has_node\": \"\"\"SELECT * FROM GRAPH_TABLE (lightrag_graph\n        MATCH (a)\n        WHERE a.workspace=:workspace AND a.name=:node_id\n        COLUMNS (a.name))\"\"\",\n    \"has_edge\": \"\"\"SELECT * FROM GRAPH_TABLE (lightrag_graph\n        MATCH (a) -[e]-> (b)\n        WHERE e.workspace=:workspace and a.workspace=:workspace and b.workspace=:workspace\n        AND a.name=:source_node_id AND b.name=:target_node_id\n        COLUMNS (e.source_name,e.target_name)  )\"\"\",\n    \"node_degree\": \"\"\"SELECT count(1) as degree FROM GRAPH_TABLE (lightrag_graph\n        MATCH (a)-[e]->(b)\n        WHERE a.workspace=:workspace and a.workspace=:workspace and b.workspace=:workspace\n        AND a.name=:node_id or b.name = :node_id\n        COLUMNS (a.name))\"\"\",\n    \"get_node\": \"\"\"SELECT t1.name,t2.entity_type,t2.source_chunk_id as source_id,NVL(t2.description,'') AS description\n        FROM GRAPH_TABLE (lightrag_graph\n        MATCH (a)\n        WHERE a.workspace=:workspace AND a.name=:node_id\n        COLUMNS (a.name)\n        ) t1 JOIN LIGHTRAG_GRAPH_NODES t2 on t1.name=t2.name\n        WHERE t2.workspace=:workspace\"\"\",\n    \"get_edge\": \"\"\"SELECT t1.source_id,t2.weight,t2.source_chunk_id as source_id,t2.keywords,\n        NVL(t2.description,'') AS description,NVL(t2.KEYWORDS,'') AS keywords\n        FROM GRAPH_TABLE (lightrag_graph\n        MATCH (a)-[e]->(b)\n        WHERE e.workspace=:workspace and a.workspace=:workspace and b.workspace=:workspace\n        AND a.name=:source_node_id and b.name = :target_node_id\n        COLUMNS (e.id,a.name as source_id)\n        ) t1 JOIN LIGHTRAG_GRAPH_EDGES t2 on t1.id=t2.id\"\"\",\n    \"get_node_edges\": \"\"\"SELECT source_name,target_name\n            FROM GRAPH_TABLE (lightrag_graph\n            MATCH (a)-[e]->(b)\n            WHERE e.workspace=:workspace and a.workspace=:workspace and b.workspace=:workspace\n            AND a.name=:source_node_id\n            COLUMNS (a.name as source_name,b.name as target_name))\"\"\",\n    \"merge_node\": \"\"\"MERGE INTO LIGHTRAG_GRAPH_NODES a\n                    USING DUAL\n                    ON (a.workspace = :workspace and a.name=:name and a.source_chunk_id=:source_chunk_id)\n                WHEN NOT MATCHED THEN\n                    INSERT(workspace,name,entity_type,description,source_chunk_id,content,content_vector)\n                    values (:workspace,:name,:entity_type,:description,:source_chunk_id,:content,:content_vector) \"\"\",\n    \"merge_edge\": \"\"\"MERGE INTO LIGHTRAG_GRAPH_EDGES a\n                    USING DUAL\n                    ON (a.workspace = :workspace and a.source_name=:source_name and a.target_name=:target_name and a.source_chunk_id=:source_chunk_id)\n                WHEN NOT MATCHED THEN\n                    INSERT(workspace,source_name,target_name,weight,keywords,description,source_chunk_id,content,content_vector)\n                    values (:workspace,:source_name,:target_name,:weight,:keywords,:description,:source_chunk_id,:content,:content_vector) \"\"\",\n    \"get_all_nodes\": \"\"\"WITH t0 AS (\n                        SELECT name AS id, entity_type AS label, entity_type, description,\n                            '[\"' || replace(source_chunk_id, '<SEP>', '\",\"') || '\"]'     source_chunk_ids\n                        FROM lightrag_graph_nodes\n                        WHERE workspace = :workspace\n                        ORDER BY createtime DESC fetch first :limit rows only\n                    ), t1 AS (\n                        SELECT t0.id, source_chunk_id\n                        FROM t0, JSON_TABLE ( source_chunk_ids, '$[*]' COLUMNS ( source_chunk_id PATH '$' ) )\n                    ), t2 AS (\n                        SELECT t1.id, LISTAGG(t2.content, '\\n') content\n                        FROM t1 LEFT JOIN lightrag_doc_chunks t2 ON t1.source_chunk_id = t2.id\n                        GROUP BY t1.id\n                    )\n                    SELECT t0.id, label, entity_type, description, t2.content\n                    FROM t0 LEFT JOIN t2 ON t0.id = t2.id\"\"\",\n    \"get_all_edges\": \"\"\"SELECT t1.id,t1.keywords as label,t1.keywords, t1.source_name as source, t1.target_name as target,\n                t1.weight,t1.DESCRIPTION,t2.content\n                FROM LIGHTRAG_GRAPH_EDGES t1\n                LEFT JOIN LIGHTRAG_DOC_CHUNKS t2 on t1.source_chunk_id=t2.id\n                WHERE t1.workspace=:workspace\n                order by t1.CREATETIME DESC\n                fetch first :limit rows only\"\"\",\n    \"get_statistics\": \"\"\"select  count(distinct CASE WHEN type='node' THEN id END) as nodes_count,\n                count(distinct CASE WHEN type='edge' THEN id END) as edges_count\n                FROM (\n                select 'node' as type, id FROM GRAPH_TABLE (lightrag_graph\n                    MATCH (a) WHERE a.workspace=:workspace columns(a.name as id))\n                UNION\n                select 'edge' as type, TO_CHAR(id) id FROM GRAPH_TABLE (lightrag_graph\n                    MATCH (a)-[e]->(b) WHERE e.workspace=:workspace columns(e.id))\n                )\"\"\",\n}"
  },
  "doc-8b16251883748a2cd9a1c078f6c88535": {
    "content": "# source: lightrag/kg/__init__.py\n# content_type: simplified_code\n# language: python\n\n# print (\"init package vars here. ......\")"
  }
}